{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# leaky relu activation, which is needed for discriminator network\n",
    "def lrelu(x, leak = 0.2, name = \"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The generator takes a vector of random numbers and transforms it into 32 * 32 image. \n",
    "Each layer in the network involves a strided transpose convolution, batch normalization, and rectified nonlinearity.\n",
    "'''\n",
    "def generator(z):\n",
    "    zP = slim.fully_connected(z, 4*4*256, normalizer_fn = slim.batch_norm,\n",
    "                              activation_fn = tf.nn.relu, \n",
    "                              scope = 'g_project', \n",
    "                              weights_initializer = initializer)\n",
    "    zCon = tf.reshape(zP, [-1, 4, 4, 256])\n",
    "    gen1 = slim.convolution2d_transpose(zCon, \n",
    "                                       num_outputs = 64,\n",
    "                                       kernel_size = [5, 5],\n",
    "                                       stride = [2, 2],\n",
    "                                       padding = \"SAME\",\n",
    "                                       normalizer_fn = slim.batch_norm,\n",
    "                                       activation_fn = tf.nn.relu,\n",
    "                                       scope = 'g_conv1',\n",
    "                                       weights_initializer = initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(gen1,\n",
    "                                       num_outputs = 32,\n",
    "                                       kernel_size = [5, 5],\n",
    "                                       stride = [2, 2],\n",
    "                                       padding = \"SAME\",\n",
    "                                       normalizer_fn = slim.batch_norm,\n",
    "                                       activation_fn = tf.nn.relu,\n",
    "                                       scope = 'g_conv2',\n",
    "                                       weights_initializer = initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(gen2,\n",
    "                                       num_outputs = 16,\n",
    "                                       kernel_size = [5, 5],\n",
    "                                       stride = [2, 2],\n",
    "                                       padding = \"SAME\",\n",
    "                                       normalizer_fn = slim.batch_norm,\n",
    "                                       activation_fn = tf.nn.relu,\n",
    "                                       scope = 'g_conv3',\n",
    "                                       weights_initializer = initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(gen3,\n",
    "                                        num_outputs = 1,\n",
    "                                        kernel_size = [32, 32],\n",
    "                                        padding = \"SAME\",\n",
    "                                        biases_initializer = None,\n",
    "                                        activation_fn = tf.nn.tanh,\n",
    "                                        scope = 'g_out',\n",
    "                                        weights_initializer = initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The disrciminator network takes as input a 32 * 32 image \n",
    "and transforms it into a single valued probability of being generated from real-world data.\n",
    "'''\n",
    "def discriminator(bottom, reuse=False):\n",
    "    \n",
    "    dis1 = slim.convolution2d(bottom,16,[4,4],\n",
    "                              stride=[2,2],\n",
    "                              padding=\"SAME\",\n",
    "                              biases_initializer=None,\n",
    "                              activation_fn=lrelu,\n",
    "                              reuse=reuse,\n",
    "                              scope='d_conv1',\n",
    "                              weights_initializer=initializer)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,32,[4,4],\n",
    "                              stride=[2,2],\n",
    "                              padding=\"SAME\",\n",
    "                              normalizer_fn=slim.batch_norm,\n",
    "                              activation_fn=lrelu,\n",
    "                              reuse=reuse,\n",
    "                              scope='d_conv2', \n",
    "                              weights_initializer=initializer)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,64,[4,4],\n",
    "                              stride=[2,2],\n",
    "                              padding=\"SAME\",\n",
    "                              normalizer_fn=slim.batch_norm,\n",
    "                              activation_fn=lrelu,\n",
    "                              reuse=reuse,\n",
    "                              scope='d_conv3',\n",
    "                              weights_initializer=initializer)\n",
    "    \n",
    "    d_out = slim.fully_connected(slim.flatten(dis3),1,\n",
    "                                 activation_fn=tf.nn.sigmoid,\n",
    "                                 reuse=reuse,\n",
    "                                 scope='d_out', \n",
    "                                 weights_initializer=initializer)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connecting them together\n",
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz, reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.33839 Disc Loss: 1.56507\n",
      "Gen Loss: 0.188747 Disc Loss: 2.42052\n",
      "Gen Loss: 0.303352 Disc Loss: 2.09308\n",
      "Gen Loss: 0.273291 Disc Loss: 2.20632\n",
      "Gen Loss: 0.438183 Disc Loss: 1.93549\n",
      "Gen Loss: 0.523456 Disc Loss: 1.5967\n",
      "Gen Loss: 0.610083 Disc Loss: 1.70257\n",
      "Gen Loss: 0.587572 Disc Loss: 1.85268\n",
      "Gen Loss: 0.633199 Disc Loss: 1.59692\n",
      "Gen Loss: 0.762366 Disc Loss: 1.41848\n",
      "Gen Loss: 0.795901 Disc Loss: 1.3607\n",
      "Gen Loss: 0.915142 Disc Loss: 1.45831\n",
      "Gen Loss: 0.934629 Disc Loss: 1.17506\n",
      "Gen Loss: 0.604913 Disc Loss: 1.68716\n",
      "Gen Loss: 0.866273 Disc Loss: 1.36201\n",
      "Gen Loss: 1.082 Disc Loss: 1.20213\n",
      "Gen Loss: 0.939154 Disc Loss: 1.20108\n",
      "Gen Loss: 0.701369 Disc Loss: 1.39765\n",
      "Gen Loss: 0.879633 Disc Loss: 1.20638\n",
      "Gen Loss: 0.747428 Disc Loss: 1.32374\n",
      "Gen Loss: 1.13968 Disc Loss: 1.1348\n",
      "Gen Loss: 1.11608 Disc Loss: 1.35967\n",
      "Gen Loss: 1.01706 Disc Loss: 1.28319\n",
      "Gen Loss: 1.15254 Disc Loss: 1.05237\n",
      "Gen Loss: 0.761783 Disc Loss: 1.5994\n",
      "Gen Loss: 0.917517 Disc Loss: 1.09024\n",
      "Gen Loss: 0.97077 Disc Loss: 1.19465\n",
      "Gen Loss: 1.04846 Disc Loss: 1.18648\n",
      "Gen Loss: 1.22174 Disc Loss: 0.906644\n",
      "Gen Loss: 0.835305 Disc Loss: 1.29784\n",
      "Gen Loss: 1.42104 Disc Loss: 0.720553\n",
      "Gen Loss: 1.052 Disc Loss: 0.954499\n",
      "Gen Loss: 1.27839 Disc Loss: 0.958251\n",
      "Gen Loss: 1.16049 Disc Loss: 0.964776\n",
      "Gen Loss: 1.30749 Disc Loss: 0.971023\n",
      "Gen Loss: 1.27056 Disc Loss: 0.595774\n",
      "Gen Loss: 0.937975 Disc Loss: 1.31024\n",
      "Gen Loss: 1.62772 Disc Loss: 0.681455\n",
      "Gen Loss: 0.59688 Disc Loss: 2.08807\n",
      "Gen Loss: 1.28488 Disc Loss: 0.904807\n",
      "Gen Loss: 0.949711 Disc Loss: 1.07558\n",
      "Gen Loss: 0.926313 Disc Loss: 1.35789\n",
      "Gen Loss: 0.922843 Disc Loss: 1.21822\n",
      "Gen Loss: 0.748872 Disc Loss: 1.29811\n",
      "Gen Loss: 0.817009 Disc Loss: 1.27167\n",
      "Gen Loss: 1.08388 Disc Loss: 1.09843\n",
      "Gen Loss: 0.902233 Disc Loss: 1.42685\n",
      "Gen Loss: 0.67961 Disc Loss: 1.55911\n",
      "Gen Loss: 0.946776 Disc Loss: 1.26336\n",
      "Gen Loss: 0.829592 Disc Loss: 1.31817\n",
      "Gen Loss: 0.843786 Disc Loss: 1.23787\n",
      "Gen Loss: 0.976115 Disc Loss: 1.14828\n",
      "Gen Loss: 0.828078 Disc Loss: 1.34955\n",
      "Gen Loss: 0.885414 Disc Loss: 1.58808\n",
      "Gen Loss: 0.880932 Disc Loss: 1.15318\n",
      "Gen Loss: 1.0878 Disc Loss: 0.926688\n",
      "Gen Loss: 1.02621 Disc Loss: 1.11114\n",
      "Gen Loss: 0.902332 Disc Loss: 1.45639\n",
      "Gen Loss: 0.899797 Disc Loss: 1.15196\n",
      "Gen Loss: 0.935719 Disc Loss: 1.26347\n",
      "Gen Loss: 0.901961 Disc Loss: 1.17045\n",
      "Gen Loss: 1.1369 Disc Loss: 0.979253\n",
      "Gen Loss: 0.871224 Disc Loss: 1.374\n",
      "Gen Loss: 0.897835 Disc Loss: 1.34309\n",
      "Gen Loss: 0.804054 Disc Loss: 1.33987\n",
      "Gen Loss: 0.902524 Disc Loss: 1.34512\n",
      "Gen Loss: 0.807589 Disc Loss: 1.31264\n",
      "Gen Loss: 0.846721 Disc Loss: 1.36267\n",
      "Gen Loss: 0.682178 Disc Loss: 1.54614\n",
      "Gen Loss: 0.911458 Disc Loss: 1.16639\n",
      "Gen Loss: 0.798875 Disc Loss: 1.52406\n",
      "Gen Loss: 0.835918 Disc Loss: 1.26874\n",
      "Gen Loss: 0.788511 Disc Loss: 1.31115\n",
      "Gen Loss: 0.87039 Disc Loss: 1.21035\n",
      "Gen Loss: 0.81294 Disc Loss: 1.37595\n",
      "Gen Loss: 0.918495 Disc Loss: 1.30223\n",
      "Gen Loss: 0.849371 Disc Loss: 1.34459\n",
      "Gen Loss: 0.876947 Disc Loss: 1.31485\n",
      "Gen Loss: 0.984798 Disc Loss: 1.04575\n",
      "Gen Loss: 0.701908 Disc Loss: 1.50134\n",
      "Gen Loss: 0.827839 Disc Loss: 1.21884\n",
      "Gen Loss: 0.863252 Disc Loss: 1.41504\n",
      "Gen Loss: 0.782106 Disc Loss: 1.25572\n",
      "Gen Loss: 0.885045 Disc Loss: 1.15512\n",
      "Gen Loss: 0.983313 Disc Loss: 1.239\n",
      "Gen Loss: 0.974125 Disc Loss: 1.21269\n",
      "Gen Loss: 0.979902 Disc Loss: 1.15678\n",
      "Gen Loss: 0.627843 Disc Loss: 1.89498\n",
      "Gen Loss: 0.861026 Disc Loss: 1.16193\n",
      "Gen Loss: 0.998659 Disc Loss: 1.0852\n",
      "Gen Loss: 0.861819 Disc Loss: 1.26316\n",
      "Gen Loss: 0.809158 Disc Loss: 1.47635\n",
      "Gen Loss: 0.825954 Disc Loss: 1.28764\n",
      "Gen Loss: 0.871678 Disc Loss: 1.32473\n",
      "Gen Loss: 0.98683 Disc Loss: 1.12752\n",
      "Gen Loss: 1.00298 Disc Loss: 1.16245\n",
      "Gen Loss: 0.798014 Disc Loss: 1.25325\n",
      "Gen Loss: 0.870504 Disc Loss: 1.32869\n",
      "Gen Loss: 0.955965 Disc Loss: 1.32555\n",
      "Gen Loss: 0.907766 Disc Loss: 1.28578\n",
      "Gen Loss: 0.855958 Disc Loss: 1.35055\n",
      "Saved Model\n",
      "Gen Loss: 0.942709 Disc Loss: 1.2901\n",
      "Gen Loss: 0.980335 Disc Loss: 1.06888\n",
      "Gen Loss: 1.17043 Disc Loss: 1.02738\n",
      "Gen Loss: 0.913892 Disc Loss: 1.46442\n",
      "Gen Loss: 0.795253 Disc Loss: 1.30949\n",
      "Gen Loss: 0.916121 Disc Loss: 1.13409\n",
      "Gen Loss: 0.928454 Disc Loss: 1.31182\n",
      "Gen Loss: 1.01657 Disc Loss: 1.16287\n",
      "Gen Loss: 0.876965 Disc Loss: 1.24908\n",
      "Gen Loss: 0.953356 Disc Loss: 1.57914\n",
      "Gen Loss: 1.07469 Disc Loss: 1.10781\n",
      "Gen Loss: 1.1062 Disc Loss: 1.12636\n",
      "Gen Loss: 1.1805 Disc Loss: 1.22045\n",
      "Gen Loss: 1.25818 Disc Loss: 1.06965\n",
      "Gen Loss: 0.841699 Disc Loss: 1.61545\n",
      "Gen Loss: 0.898545 Disc Loss: 1.24295\n",
      "Gen Loss: 1.57789 Disc Loss: 1.12932\n",
      "Gen Loss: 0.860677 Disc Loss: 1.56304\n",
      "Gen Loss: 1.02626 Disc Loss: 1.11875\n",
      "Gen Loss: 0.890896 Disc Loss: 1.293\n",
      "Gen Loss: 1.09714 Disc Loss: 1.23127\n",
      "Gen Loss: 1.32252 Disc Loss: 0.813666\n",
      "Gen Loss: 1.08236 Disc Loss: 1.03674\n",
      "Gen Loss: 1.5725 Disc Loss: 0.868304\n",
      "Gen Loss: 1.25412 Disc Loss: 0.988393\n",
      "Gen Loss: 1.12785 Disc Loss: 1.17068\n",
      "Gen Loss: 1.16792 Disc Loss: 1.10659\n",
      "Gen Loss: 1.14738 Disc Loss: 1.08468\n",
      "Gen Loss: 1.3135 Disc Loss: 1.13455\n",
      "Gen Loss: 0.806394 Disc Loss: 1.35491\n",
      "Gen Loss: 1.21448 Disc Loss: 0.883552\n",
      "Gen Loss: 1.08738 Disc Loss: 0.990523\n",
      "Gen Loss: 1.31864 Disc Loss: 1.12971\n",
      "Gen Loss: 0.975624 Disc Loss: 1.09002\n",
      "Gen Loss: 1.15888 Disc Loss: 1.29502\n",
      "Gen Loss: 1.17241 Disc Loss: 0.983442\n",
      "Gen Loss: 1.17003 Disc Loss: 1.07665\n",
      "Gen Loss: 0.984586 Disc Loss: 1.55182\n",
      "Gen Loss: 1.04895 Disc Loss: 1.08184\n",
      "Gen Loss: 1.13051 Disc Loss: 1.02955\n",
      "Gen Loss: 1.38734 Disc Loss: 0.870763\n",
      "Gen Loss: 0.785019 Disc Loss: 0.808403\n",
      "Gen Loss: 1.20922 Disc Loss: 0.943311\n",
      "Gen Loss: 0.982316 Disc Loss: 1.30412\n",
      "Gen Loss: 1.09598 Disc Loss: 1.0413\n",
      "Gen Loss: 1.04325 Disc Loss: 1.08797\n",
      "Gen Loss: 1.16371 Disc Loss: 1.05352\n",
      "Gen Loss: 1.00782 Disc Loss: 1.1477\n",
      "Gen Loss: 1.14331 Disc Loss: 1.00049\n",
      "Gen Loss: 1.42665 Disc Loss: 1.20821\n",
      "Gen Loss: 1.16516 Disc Loss: 0.920217\n",
      "Gen Loss: 1.19175 Disc Loss: 1.04491\n",
      "Gen Loss: 1.32332 Disc Loss: 0.76003\n",
      "Gen Loss: 1.07657 Disc Loss: 1.11418\n",
      "Gen Loss: 1.24364 Disc Loss: 0.9422\n",
      "Gen Loss: 0.864671 Disc Loss: 1.29585\n",
      "Gen Loss: 1.20853 Disc Loss: 1.1313\n",
      "Gen Loss: 1.49143 Disc Loss: 0.973098\n",
      "Gen Loss: 1.29389 Disc Loss: 1.09604\n",
      "Gen Loss: 1.08131 Disc Loss: 1.37053\n",
      "Gen Loss: 1.00802 Disc Loss: 1.17723\n",
      "Gen Loss: 1.29096 Disc Loss: 1.19572\n",
      "Gen Loss: 1.21932 Disc Loss: 1.18844\n",
      "Gen Loss: 1.17616 Disc Loss: 1.07786\n",
      "Gen Loss: 1.08597 Disc Loss: 1.10585\n",
      "Gen Loss: 1.10424 Disc Loss: 1.06992\n",
      "Gen Loss: 1.17527 Disc Loss: 1.05613\n",
      "Gen Loss: 1.165 Disc Loss: 1.02993\n",
      "Gen Loss: 1.14171 Disc Loss: 1.06211\n",
      "Gen Loss: 1.19471 Disc Loss: 1.07537\n",
      "Gen Loss: 1.10606 Disc Loss: 0.91227\n",
      "Gen Loss: 1.21518 Disc Loss: 1.02566\n",
      "Gen Loss: 0.97808 Disc Loss: 1.15528\n",
      "Gen Loss: 1.23319 Disc Loss: 1.02693\n",
      "Gen Loss: 1.73089 Disc Loss: 0.928554\n",
      "Gen Loss: 1.18448 Disc Loss: 1.06518\n",
      "Gen Loss: 1.08972 Disc Loss: 1.31779\n",
      "Gen Loss: 1.24212 Disc Loss: 0.83854\n",
      "Gen Loss: 1.22909 Disc Loss: 1.13638\n",
      "Gen Loss: 1.13527 Disc Loss: 0.9978\n",
      "Gen Loss: 1.31926 Disc Loss: 0.913454\n",
      "Gen Loss: 1.26785 Disc Loss: 1.10289\n",
      "Gen Loss: 1.56304 Disc Loss: 0.815903\n",
      "Gen Loss: 1.72698 Disc Loss: 0.56533\n",
      "Gen Loss: 1.56291 Disc Loss: 0.712729\n",
      "Gen Loss: 0.720185 Disc Loss: 0.793597\n",
      "Gen Loss: 1.55955 Disc Loss: 1.02928\n",
      "Gen Loss: 1.52958 Disc Loss: 0.69484\n",
      "Gen Loss: 1.25238 Disc Loss: 0.972253\n",
      "Gen Loss: 1.18835 Disc Loss: 0.981268\n",
      "Gen Loss: 1.65566 Disc Loss: 1.08129\n",
      "Gen Loss: 1.09559 Disc Loss: 0.952181\n",
      "Gen Loss: 0.926941 Disc Loss: 0.767266\n",
      "Gen Loss: 1.00667 Disc Loss: 1.32004\n",
      "Gen Loss: 1.36124 Disc Loss: 0.864607\n",
      "Gen Loss: 1.65895 Disc Loss: 1.35846\n",
      "Gen Loss: 1.04425 Disc Loss: 1.19396\n",
      "Gen Loss: 1.30642 Disc Loss: 1.10117\n",
      "Gen Loss: 1.28591 Disc Loss: 0.851202\n",
      "Gen Loss: 1.22941 Disc Loss: 0.875808\n",
      "Saved Model\n",
      "Gen Loss: 1.44044 Disc Loss: 0.767967\n",
      "Gen Loss: 1.8145 Disc Loss: 1.12666\n",
      "Gen Loss: 1.65659 Disc Loss: 0.587751\n",
      "Gen Loss: 0.402886 Disc Loss: 0.749355\n",
      "Gen Loss: 1.16911 Disc Loss: 0.807645\n",
      "Gen Loss: 1.11844 Disc Loss: 1.23061\n",
      "Gen Loss: 1.17762 Disc Loss: 0.91614\n",
      "Gen Loss: 1.87008 Disc Loss: 0.523629\n",
      "Gen Loss: 1.09698 Disc Loss: 1.23442\n",
      "Gen Loss: 1.59124 Disc Loss: 0.726081\n",
      "Gen Loss: 1.68327 Disc Loss: 0.512889\n",
      "Gen Loss: 1.59271 Disc Loss: 0.69897\n",
      "Gen Loss: 1.60024 Disc Loss: 0.803339\n",
      "Gen Loss: 1.78084 Disc Loss: 0.680071\n",
      "Gen Loss: 1.53867 Disc Loss: 0.789515\n",
      "Gen Loss: 1.28758 Disc Loss: 1.45091\n",
      "Gen Loss: 1.43232 Disc Loss: 0.771472\n",
      "Gen Loss: 1.70405 Disc Loss: 0.689551\n",
      "Gen Loss: 1.88343 Disc Loss: 0.547015\n",
      "Gen Loss: 1.48891 Disc Loss: 0.693837\n",
      "Gen Loss: 2.20907 Disc Loss: 0.818947\n",
      "Gen Loss: 1.53773 Disc Loss: 0.684181\n",
      "Gen Loss: 1.808 Disc Loss: 0.868669\n",
      "Gen Loss: 1.64514 Disc Loss: 0.706515\n",
      "Gen Loss: 1.76749 Disc Loss: 0.624415\n",
      "Gen Loss: 1.82154 Disc Loss: 0.686308\n",
      "Gen Loss: 1.58288 Disc Loss: 0.58034\n",
      "Gen Loss: 1.62798 Disc Loss: 0.527789\n",
      "Gen Loss: 1.74218 Disc Loss: 0.727383\n",
      "Gen Loss: 1.82445 Disc Loss: 0.5079\n",
      "Gen Loss: 2.22176 Disc Loss: 0.750472\n",
      "Gen Loss: 0.952749 Disc Loss: 0.931388\n",
      "Gen Loss: 2.08614 Disc Loss: 0.41247\n",
      "Gen Loss: 1.99338 Disc Loss: 0.663571\n",
      "Gen Loss: 0.449401 Disc Loss: 1.24004\n",
      "Gen Loss: 1.39311 Disc Loss: 1.20416\n",
      "Gen Loss: 1.6782 Disc Loss: 0.765442\n",
      "Gen Loss: 1.97958 Disc Loss: 0.703889\n",
      "Gen Loss: 1.59894 Disc Loss: 0.564403\n",
      "Gen Loss: 1.18118 Disc Loss: 1.07506\n",
      "Gen Loss: 1.27478 Disc Loss: 1.16922\n",
      "Gen Loss: 1.43487 Disc Loss: 0.605606\n",
      "Gen Loss: 1.30746 Disc Loss: 0.827427\n",
      "Gen Loss: 1.70464 Disc Loss: 0.808547\n",
      "Gen Loss: 1.49778 Disc Loss: 0.76989\n",
      "Gen Loss: 1.59074 Disc Loss: 1.02078\n",
      "Gen Loss: 1.4115 Disc Loss: 0.782097\n",
      "Gen Loss: 1.97275 Disc Loss: 0.775349\n",
      "Gen Loss: 1.81642 Disc Loss: 0.916891\n",
      "Gen Loss: 1.13529 Disc Loss: 0.814029\n",
      "Gen Loss: 1.65028 Disc Loss: 0.663862\n",
      "Gen Loss: 1.7323 Disc Loss: 0.663877\n",
      "Gen Loss: 1.36711 Disc Loss: 0.859455\n",
      "Gen Loss: 1.68104 Disc Loss: 0.655255\n",
      "Gen Loss: 1.57797 Disc Loss: 0.717678\n",
      "Gen Loss: 1.69152 Disc Loss: 0.872868\n",
      "Gen Loss: 1.37503 Disc Loss: 0.789232\n",
      "Gen Loss: 1.58377 Disc Loss: 0.702861\n",
      "Gen Loss: 1.88364 Disc Loss: 1.01964\n",
      "Gen Loss: 1.27057 Disc Loss: 0.560766\n",
      "Gen Loss: 1.56189 Disc Loss: 0.998542\n",
      "Gen Loss: 1.55416 Disc Loss: 0.591186\n",
      "Gen Loss: 1.90008 Disc Loss: 0.505141\n",
      "Gen Loss: 1.93223 Disc Loss: 0.548757\n",
      "Gen Loss: 1.75456 Disc Loss: 0.903668\n",
      "Gen Loss: 1.93868 Disc Loss: 0.858324\n",
      "Gen Loss: 1.42701 Disc Loss: 0.872545\n",
      "Gen Loss: 1.2954 Disc Loss: 1.0141\n",
      "Gen Loss: 1.51562 Disc Loss: 0.538862\n",
      "Gen Loss: 1.9318 Disc Loss: 0.750629\n",
      "Gen Loss: 1.75149 Disc Loss: 0.807841\n",
      "Gen Loss: 1.43536 Disc Loss: 0.78262\n",
      "Gen Loss: 1.88077 Disc Loss: 0.56328\n",
      "Gen Loss: 2.17334 Disc Loss: 1.15241\n",
      "Gen Loss: 0.761926 Disc Loss: 0.764936\n",
      "Gen Loss: 1.59264 Disc Loss: 0.703413\n",
      "Gen Loss: 1.95439 Disc Loss: 0.408274\n",
      "Gen Loss: 1.48364 Disc Loss: 0.682118\n",
      "Gen Loss: 2.0774 Disc Loss: 0.410585\n",
      "Gen Loss: 1.66525 Disc Loss: 0.542095\n",
      "Gen Loss: 2.06958 Disc Loss: 0.622028\n",
      "Gen Loss: 1.80259 Disc Loss: 0.528467\n",
      "Gen Loss: 1.68322 Disc Loss: 0.823545\n",
      "Gen Loss: 1.80341 Disc Loss: 0.53565\n",
      "Gen Loss: 1.852 Disc Loss: 0.516706\n",
      "Gen Loss: 1.35061 Disc Loss: 0.708023\n",
      "Gen Loss: 1.19395 Disc Loss: 0.761786\n",
      "Gen Loss: 2.11979 Disc Loss: 0.714423\n",
      "Gen Loss: 2.62732 Disc Loss: 0.830704\n",
      "Gen Loss: 1.56002 Disc Loss: 0.717297\n",
      "Gen Loss: 1.41861 Disc Loss: 0.835341\n",
      "Gen Loss: 1.52489 Disc Loss: 0.756608\n",
      "Gen Loss: 1.56189 Disc Loss: 0.580623\n",
      "Gen Loss: 1.23641 Disc Loss: 0.98652\n",
      "Gen Loss: 1.21409 Disc Loss: 0.813389\n",
      "Gen Loss: 1.79571 Disc Loss: 0.488012\n",
      "Gen Loss: 1.69848 Disc Loss: 0.552188\n",
      "Gen Loss: 2.07913 Disc Loss: 0.709232\n",
      "Gen Loss: 2.08295 Disc Loss: 0.526203\n",
      "Gen Loss: 1.68752 Disc Loss: 0.639449\n",
      "Saved Model\n",
      "Gen Loss: 1.12115 Disc Loss: 0.653257\n",
      "Gen Loss: 2.14013 Disc Loss: 0.371722\n",
      "Gen Loss: 2.09275 Disc Loss: 0.625297\n",
      "Gen Loss: 1.59561 Disc Loss: 0.607408\n",
      "Gen Loss: 2.78558 Disc Loss: 1.18046\n",
      "Gen Loss: 1.39689 Disc Loss: 0.863277\n",
      "Gen Loss: 1.82707 Disc Loss: 0.682939\n",
      "Gen Loss: 1.59631 Disc Loss: 0.614123\n",
      "Gen Loss: 1.74812 Disc Loss: 0.489075\n",
      "Gen Loss: 1.85231 Disc Loss: 0.683389\n",
      "Gen Loss: 1.24143 Disc Loss: 1.10002\n",
      "Gen Loss: 1.82986 Disc Loss: 0.700633\n",
      "Gen Loss: 1.35372 Disc Loss: 0.698952\n",
      "Gen Loss: 1.9162 Disc Loss: 1.10237\n",
      "Gen Loss: 1.71687 Disc Loss: 0.704853\n",
      "Gen Loss: 1.82218 Disc Loss: 0.588616\n",
      "Gen Loss: 2.05834 Disc Loss: 0.725921\n",
      "Gen Loss: 1.8894 Disc Loss: 0.600388\n",
      "Gen Loss: 1.40753 Disc Loss: 0.923214\n",
      "Gen Loss: 2.11905 Disc Loss: 0.756724\n",
      "Gen Loss: 1.94052 Disc Loss: 1.01332\n",
      "Gen Loss: 1.67335 Disc Loss: 0.91818\n",
      "Gen Loss: 1.54425 Disc Loss: 0.80247\n",
      "Gen Loss: 2.14817 Disc Loss: 0.604247\n",
      "Gen Loss: 1.72316 Disc Loss: 0.661644\n",
      "Gen Loss: 2.56006 Disc Loss: 0.426145\n",
      "Gen Loss: 2.02389 Disc Loss: 0.44659\n",
      "Gen Loss: 2.52077 Disc Loss: 0.568957\n",
      "Gen Loss: 1.98277 Disc Loss: 0.554999\n",
      "Gen Loss: 2.67028 Disc Loss: 1.48782\n",
      "Gen Loss: 2.50856 Disc Loss: 0.377092\n",
      "Gen Loss: 1.80625 Disc Loss: 0.529294\n",
      "Gen Loss: 2.1555 Disc Loss: 0.707966\n",
      "Gen Loss: 1.32784 Disc Loss: 0.622253\n",
      "Gen Loss: 1.88142 Disc Loss: 0.619909\n",
      "Gen Loss: 1.58596 Disc Loss: 0.507985\n",
      "Gen Loss: 1.04667 Disc Loss: 0.735697\n",
      "Gen Loss: 1.75129 Disc Loss: 0.645908\n",
      "Gen Loss: 2.04666 Disc Loss: 0.619669\n",
      "Gen Loss: 1.90085 Disc Loss: 0.527315\n",
      "Gen Loss: 1.89881 Disc Loss: 0.700478\n",
      "Gen Loss: 0.952029 Disc Loss: 0.762127\n",
      "Gen Loss: 1.54236 Disc Loss: 0.58532\n",
      "Gen Loss: 2.43071 Disc Loss: 0.707643\n",
      "Gen Loss: 2.4265 Disc Loss: 0.396809\n",
      "Gen Loss: 1.72057 Disc Loss: 0.431577\n",
      "Gen Loss: 1.48887 Disc Loss: 0.791487\n",
      "Gen Loss: 2.26416 Disc Loss: 0.61815\n",
      "Gen Loss: 1.68748 Disc Loss: 0.616739\n",
      "Gen Loss: 1.55067 Disc Loss: 0.741776\n",
      "Gen Loss: 1.84204 Disc Loss: 0.529362\n",
      "Gen Loss: 2.09382 Disc Loss: 0.598195\n",
      "Gen Loss: 1.2504 Disc Loss: 0.864747\n",
      "Gen Loss: 1.60871 Disc Loss: 0.590881\n",
      "Gen Loss: 1.47088 Disc Loss: 0.596904\n",
      "Gen Loss: 1.69394 Disc Loss: 0.717558\n",
      "Gen Loss: 2.06376 Disc Loss: 0.50095\n",
      "Gen Loss: 2.31527 Disc Loss: 0.624042\n",
      "Gen Loss: 1.78323 Disc Loss: 0.689919\n",
      "Gen Loss: 1.83699 Disc Loss: 0.544587\n",
      "Gen Loss: 1.91658 Disc Loss: 0.905457\n",
      "Gen Loss: 1.33753 Disc Loss: 0.582777\n",
      "Gen Loss: 2.04518 Disc Loss: 0.497597\n",
      "Gen Loss: 2.26222 Disc Loss: 0.345888\n",
      "Gen Loss: 1.3787 Disc Loss: 0.508063\n",
      "Gen Loss: 1.38121 Disc Loss: 0.466767\n",
      "Gen Loss: 1.96828 Disc Loss: 0.54161\n",
      "Gen Loss: 1.52896 Disc Loss: 0.559339\n",
      "Gen Loss: 1.62629 Disc Loss: 0.64144\n",
      "Gen Loss: 1.50298 Disc Loss: 0.520801\n",
      "Gen Loss: 2.13566 Disc Loss: 0.613345\n",
      "Gen Loss: 1.85477 Disc Loss: 0.602488\n",
      "Gen Loss: 2.37906 Disc Loss: 0.652533\n",
      "Gen Loss: 1.67759 Disc Loss: 1.34198\n",
      "Gen Loss: 1.90971 Disc Loss: 0.748396\n",
      "Gen Loss: 2.08785 Disc Loss: 0.516069\n",
      "Gen Loss: 1.4652 Disc Loss: 0.670579\n",
      "Gen Loss: 1.63753 Disc Loss: 0.805277\n",
      "Gen Loss: 1.91006 Disc Loss: 0.493976\n",
      "Gen Loss: 2.20856 Disc Loss: 0.342962\n",
      "Gen Loss: 1.71539 Disc Loss: 0.622523\n",
      "Gen Loss: 1.21006 Disc Loss: 0.962301\n",
      "Gen Loss: 2.11953 Disc Loss: 0.423732\n",
      "Gen Loss: 2.22583 Disc Loss: 0.355801\n",
      "Gen Loss: 2.71321 Disc Loss: 0.796125\n",
      "Gen Loss: 2.48668 Disc Loss: 0.401366\n",
      "Gen Loss: 1.9323 Disc Loss: 0.6949\n",
      "Gen Loss: 1.75024 Disc Loss: 0.6606\n",
      "Gen Loss: 1.98472 Disc Loss: 0.671598\n",
      "Gen Loss: 2.33166 Disc Loss: 0.922133\n",
      "Gen Loss: 1.28344 Disc Loss: 0.875841\n",
      "Gen Loss: 1.45208 Disc Loss: 0.716807\n",
      "Gen Loss: 1.5179 Disc Loss: 0.971972\n",
      "Gen Loss: 1.95493 Disc Loss: 0.798562\n",
      "Gen Loss: 1.95783 Disc Loss: 0.659077\n",
      "Gen Loss: 1.79288 Disc Loss: 1.04675\n",
      "Gen Loss: 2.02248 Disc Loss: 0.602538\n",
      "Gen Loss: 1.67858 Disc Loss: 0.582031\n",
      "Gen Loss: 1.6937 Disc Loss: 0.691277\n",
      "Gen Loss: 1.69403 Disc Loss: 0.787715\n",
      "Saved Model\n",
      "Gen Loss: 2.28817 Disc Loss: 0.443516\n",
      "Gen Loss: 1.21535 Disc Loss: 0.601284\n",
      "Gen Loss: 1.77854 Disc Loss: 0.646143\n",
      "Gen Loss: 2.14387 Disc Loss: 0.523005\n",
      "Gen Loss: 2.15325 Disc Loss: 0.565407\n",
      "Gen Loss: 1.6315 Disc Loss: 0.606669\n",
      "Gen Loss: 1.90542 Disc Loss: 0.571226\n",
      "Gen Loss: 1.48703 Disc Loss: 0.606556\n",
      "Gen Loss: 1.27147 Disc Loss: 0.952765\n",
      "Gen Loss: 1.30066 Disc Loss: 0.693082\n",
      "Gen Loss: 1.39917 Disc Loss: 0.756106\n",
      "Gen Loss: 1.81152 Disc Loss: 0.560948\n",
      "Gen Loss: 1.8193 Disc Loss: 0.552956\n",
      "Gen Loss: 1.96807 Disc Loss: 0.415649\n",
      "Gen Loss: 1.72567 Disc Loss: 0.51438\n",
      "Gen Loss: 1.9653 Disc Loss: 0.495589\n",
      "Gen Loss: 2.3661 Disc Loss: 0.512645\n",
      "Gen Loss: 2.60033 Disc Loss: 0.310089\n",
      "Gen Loss: 1.77896 Disc Loss: 0.620703\n",
      "Gen Loss: 1.61161 Disc Loss: 0.733719\n",
      "Gen Loss: 1.85463 Disc Loss: 0.697138\n",
      "Gen Loss: 2.0577 Disc Loss: 0.402335\n",
      "Gen Loss: 2.14433 Disc Loss: 0.380888\n",
      "Gen Loss: 2.37452 Disc Loss: 0.42967\n",
      "Gen Loss: 3.30882 Disc Loss: 1.06891\n",
      "Gen Loss: 2.02235 Disc Loss: 0.604601\n",
      "Gen Loss: 3.15092 Disc Loss: 0.453485\n",
      "Gen Loss: 2.24987 Disc Loss: 0.470284\n",
      "Gen Loss: 1.76445 Disc Loss: 0.485077\n",
      "Gen Loss: 1.52172 Disc Loss: 0.670857\n",
      "Gen Loss: 1.61402 Disc Loss: 0.527291\n",
      "Gen Loss: 1.34745 Disc Loss: 0.849421\n",
      "Gen Loss: 1.96239 Disc Loss: 1.09518\n",
      "Gen Loss: 1.55339 Disc Loss: 0.738386\n",
      "Gen Loss: 1.40804 Disc Loss: 0.675132\n",
      "Gen Loss: 2.50649 Disc Loss: 0.702061\n",
      "Gen Loss: 1.62363 Disc Loss: 0.188877\n",
      "Gen Loss: 2.69635 Disc Loss: 0.657231\n",
      "Gen Loss: 1.85086 Disc Loss: 0.606492\n",
      "Gen Loss: 1.82765 Disc Loss: 0.580216\n",
      "Gen Loss: 1.96847 Disc Loss: 0.419775\n",
      "Gen Loss: 2.02472 Disc Loss: 0.411346\n",
      "Gen Loss: 2.62967 Disc Loss: 0.398373\n",
      "Gen Loss: 1.67781 Disc Loss: 0.662012\n",
      "Gen Loss: 1.31335 Disc Loss: 0.748202\n",
      "Gen Loss: 2.10114 Disc Loss: 0.643221\n",
      "Gen Loss: 1.12232 Disc Loss: 0.718269\n",
      "Gen Loss: 2.12967 Disc Loss: 0.545657\n",
      "Gen Loss: 2.07389 Disc Loss: 0.578803\n",
      "Gen Loss: 1.17203 Disc Loss: 0.985699\n",
      "Gen Loss: 1.13769 Disc Loss: 0.784696\n",
      "Gen Loss: 2.50801 Disc Loss: 0.405144\n",
      "Gen Loss: 1.33492 Disc Loss: 0.668521\n",
      "Gen Loss: 1.45 Disc Loss: 0.812277\n",
      "Gen Loss: 1.98585 Disc Loss: 0.60052\n",
      "Gen Loss: 2.19648 Disc Loss: 0.609845\n",
      "Gen Loss: 1.68757 Disc Loss: 0.650938\n",
      "Gen Loss: 2.3573 Disc Loss: 0.828084\n",
      "Gen Loss: 0.90953 Disc Loss: 0.977617\n",
      "Gen Loss: 2.0207 Disc Loss: 0.60975\n",
      "Gen Loss: 1.71881 Disc Loss: 0.619083\n",
      "Gen Loss: 0.968474 Disc Loss: 0.739413\n",
      "Gen Loss: 2.03951 Disc Loss: 0.34083\n",
      "Gen Loss: 1.92388 Disc Loss: 0.587858\n",
      "Gen Loss: 1.53341 Disc Loss: 0.559439\n",
      "Gen Loss: 1.98003 Disc Loss: 0.427847\n",
      "Gen Loss: 1.85948 Disc Loss: 0.560491\n",
      "Gen Loss: 1.66145 Disc Loss: 0.513629\n",
      "Gen Loss: 1.70156 Disc Loss: 0.8992\n",
      "Gen Loss: 1.86154 Disc Loss: 0.51016\n",
      "Gen Loss: 1.57247 Disc Loss: 0.446422\n",
      "Gen Loss: 2.1413 Disc Loss: 0.446706\n",
      "Gen Loss: 1.53257 Disc Loss: 0.608576\n",
      "Gen Loss: 1.4207 Disc Loss: 0.741173\n",
      "Gen Loss: 0.956207 Disc Loss: 0.919803\n",
      "Gen Loss: 1.46676 Disc Loss: 0.842958\n",
      "Gen Loss: 2.0556 Disc Loss: 0.723799\n",
      "Gen Loss: 1.28156 Disc Loss: 0.916916\n",
      "Gen Loss: 2.07334 Disc Loss: 0.910481\n",
      "Gen Loss: 1.4194 Disc Loss: 0.899682\n",
      "Gen Loss: 1.73188 Disc Loss: 0.740863\n",
      "Gen Loss: 2.32695 Disc Loss: 0.904164\n",
      "Gen Loss: 0.927231 Disc Loss: 0.685142\n",
      "Gen Loss: 1.66452 Disc Loss: 0.576939\n",
      "Gen Loss: 1.68068 Disc Loss: 0.521133\n",
      "Gen Loss: 1.98554 Disc Loss: 0.574133\n",
      "Gen Loss: 1.15538 Disc Loss: 0.873807\n",
      "Gen Loss: 1.70969 Disc Loss: 0.503347\n",
      "Gen Loss: 1.69969 Disc Loss: 0.903524\n",
      "Gen Loss: 1.887 Disc Loss: 0.496578\n",
      "Gen Loss: 1.11647 Disc Loss: 1.0563\n",
      "Gen Loss: 1.98049 Disc Loss: 0.446249\n",
      "Gen Loss: 1.73141 Disc Loss: 0.625509\n",
      "Gen Loss: 1.86077 Disc Loss: 0.369671\n",
      "Gen Loss: 1.55849 Disc Loss: 0.683046\n",
      "Gen Loss: 2.1032 Disc Loss: 0.744968\n",
      "Gen Loss: 0.46488 Disc Loss: 1.12645\n",
      "Gen Loss: 1.51902 Disc Loss: 0.477987\n",
      "Gen Loss: 2.00726 Disc Loss: 0.598787\n",
      "Gen Loss: 1.38024 Disc Loss: 0.54805\n",
      "Saved Model\n",
      "Gen Loss: 2.77116 Disc Loss: 0.551298\n",
      "Gen Loss: 2.04178 Disc Loss: 0.406053\n",
      "Gen Loss: 2.72621 Disc Loss: 1.03822\n",
      "Gen Loss: 1.61979 Disc Loss: 0.611695\n",
      "Gen Loss: 2.04167 Disc Loss: 0.6981\n",
      "Gen Loss: 1.52546 Disc Loss: 0.494391\n",
      "Gen Loss: 1.59209 Disc Loss: 0.83498\n",
      "Gen Loss: 1.51317 Disc Loss: 0.821404\n",
      "Gen Loss: 0.934324 Disc Loss: 0.994877\n",
      "Gen Loss: 2.03173 Disc Loss: 0.971447\n",
      "Gen Loss: 1.54676 Disc Loss: 0.723053\n",
      "Gen Loss: 0.720045 Disc Loss: 1.09446\n",
      "Gen Loss: 1.55092 Disc Loss: 0.749646\n",
      "Gen Loss: 1.78917 Disc Loss: 0.509675\n",
      "Gen Loss: 1.82885 Disc Loss: 0.442301\n",
      "Gen Loss: 1.5179 Disc Loss: 0.70118\n",
      "Gen Loss: 1.69483 Disc Loss: 0.562065\n",
      "Gen Loss: 2.54146 Disc Loss: 0.922427\n",
      "Gen Loss: 1.28419 Disc Loss: 0.82275\n",
      "Gen Loss: 1.83528 Disc Loss: 0.615193\n",
      "Gen Loss: 1.77252 Disc Loss: 0.743216\n",
      "Gen Loss: 2.2315 Disc Loss: 0.430553\n",
      "Gen Loss: 2.03141 Disc Loss: 0.386875\n",
      "Gen Loss: 2.20655 Disc Loss: 0.410387\n",
      "Gen Loss: 1.84149 Disc Loss: 0.508778\n",
      "Gen Loss: 1.83561 Disc Loss: 0.462459\n",
      "Gen Loss: 1.61807 Disc Loss: 0.412793\n",
      "Gen Loss: 1.94412 Disc Loss: 0.466722\n",
      "Gen Loss: 1.76781 Disc Loss: 0.551341\n",
      "Gen Loss: 1.75768 Disc Loss: 0.741309\n",
      "Gen Loss: 1.36377 Disc Loss: 0.691379\n",
      "Gen Loss: 1.48937 Disc Loss: 0.883214\n",
      "Gen Loss: 1.32178 Disc Loss: 0.581536\n",
      "Gen Loss: 1.49834 Disc Loss: 0.794829\n",
      "Gen Loss: 1.62103 Disc Loss: 0.525616\n",
      "Gen Loss: 1.55569 Disc Loss: 0.550765\n",
      "Gen Loss: 2.60282 Disc Loss: 0.741161\n",
      "Gen Loss: 1.47438 Disc Loss: 1.1165\n",
      "Gen Loss: 1.66309 Disc Loss: 0.740309\n",
      "Gen Loss: 1.44089 Disc Loss: 0.68699\n",
      "Gen Loss: 1.55555 Disc Loss: 0.734421\n",
      "Gen Loss: 1.2831 Disc Loss: 0.919109\n",
      "Gen Loss: 0.711935 Disc Loss: 0.990412\n",
      "Gen Loss: 1.39755 Disc Loss: 0.615566\n",
      "Gen Loss: 1.87761 Disc Loss: 0.65626\n",
      "Gen Loss: 1.66771 Disc Loss: 1.19106\n",
      "Gen Loss: 2.00638 Disc Loss: 0.652481\n",
      "Gen Loss: 1.49017 Disc Loss: 0.610961\n",
      "Gen Loss: 1.60998 Disc Loss: 0.744052\n",
      "Gen Loss: 1.37316 Disc Loss: 0.608221\n",
      "Gen Loss: 1.39713 Disc Loss: 0.706226\n",
      "Gen Loss: 1.85333 Disc Loss: 0.573804\n",
      "Gen Loss: 1.65076 Disc Loss: 0.52582\n",
      "Gen Loss: 1.40043 Disc Loss: 0.782634\n",
      "Gen Loss: 1.38717 Disc Loss: 0.70088\n",
      "Gen Loss: 1.62314 Disc Loss: 0.603489\n",
      "Gen Loss: 0.891559 Disc Loss: 0.817219\n",
      "Gen Loss: 1.37175 Disc Loss: 0.783506\n",
      "Gen Loss: 1.5267 Disc Loss: 0.728927\n",
      "Gen Loss: 0.766935 Disc Loss: 0.832382\n",
      "Gen Loss: 1.14027 Disc Loss: 0.727061\n",
      "Gen Loss: 1.07434 Disc Loss: 0.887069\n",
      "Gen Loss: 1.91792 Disc Loss: 0.882348\n",
      "Gen Loss: 1.39236 Disc Loss: 0.883724\n",
      "Gen Loss: 2.26974 Disc Loss: 0.679699\n",
      "Gen Loss: 1.45957 Disc Loss: 0.699914\n",
      "Gen Loss: 1.56577 Disc Loss: 0.583038\n",
      "Gen Loss: 1.63861 Disc Loss: 0.825235\n",
      "Gen Loss: 1.25694 Disc Loss: 0.901058\n",
      "Gen Loss: 0.867686 Disc Loss: 1.21049\n",
      "Gen Loss: 1.71442 Disc Loss: 0.610323\n",
      "Gen Loss: 1.82786 Disc Loss: 0.684983\n",
      "Gen Loss: 1.81114 Disc Loss: 0.519075\n",
      "Gen Loss: 1.78421 Disc Loss: 0.576203\n",
      "Gen Loss: 1.66491 Disc Loss: 0.684102\n",
      "Gen Loss: 1.13273 Disc Loss: 0.584964\n",
      "Gen Loss: 2.23995 Disc Loss: 0.623718\n",
      "Gen Loss: 1.84805 Disc Loss: 0.830211\n",
      "Gen Loss: 1.90691 Disc Loss: 0.62683\n",
      "Gen Loss: 2.16368 Disc Loss: 0.497235\n",
      "Gen Loss: 1.84467 Disc Loss: 0.493483\n",
      "Gen Loss: 1.28406 Disc Loss: 0.488026\n",
      "Gen Loss: 1.59891 Disc Loss: 0.434561\n",
      "Gen Loss: 2.65725 Disc Loss: 0.577738\n",
      "Gen Loss: 1.77145 Disc Loss: 0.50087\n",
      "Gen Loss: 2.11782 Disc Loss: 0.394802\n",
      "Gen Loss: 2.31785 Disc Loss: 0.421146\n",
      "Gen Loss: 1.97638 Disc Loss: 0.429955\n",
      "Gen Loss: 1.18206 Disc Loss: 0.714331\n",
      "Gen Loss: 3.06362 Disc Loss: 0.435461\n",
      "Gen Loss: 1.56785 Disc Loss: 0.542705\n",
      "Gen Loss: 1.04081 Disc Loss: 0.995825\n",
      "Gen Loss: 1.93874 Disc Loss: 0.694211\n",
      "Gen Loss: 2.01067 Disc Loss: 0.476726\n",
      "Gen Loss: 1.9559 Disc Loss: 0.474286\n",
      "Gen Loss: 1.48199 Disc Loss: 0.858361\n",
      "Gen Loss: 2.68879 Disc Loss: 0.698086\n",
      "Gen Loss: 2.05351 Disc Loss: 0.582685\n",
      "Gen Loss: 1.71245 Disc Loss: 0.465004\n",
      "Gen Loss: 2.08896 Disc Loss: 0.503093\n",
      "Saved Model\n",
      "Gen Loss: 2.1166 Disc Loss: 0.463351\n",
      "Gen Loss: 1.82402 Disc Loss: 0.515534\n",
      "Gen Loss: 1.78658 Disc Loss: 0.556822\n",
      "Gen Loss: 2.07966 Disc Loss: 0.607031\n",
      "Gen Loss: 1.20757 Disc Loss: 0.49035\n",
      "Gen Loss: 1.28449 Disc Loss: 0.722655\n",
      "Gen Loss: 1.30087 Disc Loss: 0.610005\n",
      "Gen Loss: 1.79431 Disc Loss: 0.447134\n",
      "Gen Loss: 1.44597 Disc Loss: 0.689043\n",
      "Gen Loss: 1.74619 Disc Loss: 0.446866\n",
      "Gen Loss: 1.97371 Disc Loss: 0.579245\n",
      "Gen Loss: 1.59083 Disc Loss: 0.457527\n",
      "Gen Loss: 2.11304 Disc Loss: 0.47502\n",
      "Gen Loss: 2.30179 Disc Loss: 0.573\n",
      "Gen Loss: 1.89921 Disc Loss: 0.498395\n",
      "Gen Loss: 1.95018 Disc Loss: 0.657043\n",
      "Gen Loss: 1.62741 Disc Loss: 0.611808\n",
      "Gen Loss: 1.77685 Disc Loss: 0.475405\n",
      "Gen Loss: 2.48142 Disc Loss: 0.327388\n",
      "Gen Loss: 1.67791 Disc Loss: 0.484114\n",
      "Gen Loss: 2.3972 Disc Loss: 0.686916\n",
      "Gen Loss: 1.39275 Disc Loss: 0.686573\n",
      "Gen Loss: 1.76172 Disc Loss: 0.466416\n",
      "Gen Loss: 2.13648 Disc Loss: 0.33459\n",
      "Gen Loss: 1.86783 Disc Loss: 0.59158\n",
      "Gen Loss: 1.49098 Disc Loss: 0.625158\n",
      "Gen Loss: 2.98295 Disc Loss: 1.23151\n",
      "Gen Loss: 2.04138 Disc Loss: 0.703332\n",
      "Gen Loss: 1.51525 Disc Loss: 0.754657\n",
      "Gen Loss: 2.08072 Disc Loss: 0.337195\n",
      "Gen Loss: 3.22269 Disc Loss: 0.687017\n",
      "Gen Loss: 1.46602 Disc Loss: 0.769446\n",
      "Gen Loss: 1.40353 Disc Loss: 0.505847\n",
      "Gen Loss: 2.29951 Disc Loss: 0.346582\n",
      "Gen Loss: 1.85158 Disc Loss: 0.564458\n",
      "Gen Loss: 2.03538 Disc Loss: 0.413042\n",
      "Gen Loss: 2.32168 Disc Loss: 0.422709\n",
      "Gen Loss: 1.24662 Disc Loss: 0.651661\n",
      "Gen Loss: 2.05409 Disc Loss: 0.75244\n",
      "Gen Loss: 1.99078 Disc Loss: 0.487961\n",
      "Gen Loss: 1.75591 Disc Loss: 0.597336\n",
      "Gen Loss: 1.97127 Disc Loss: 0.641038\n",
      "Gen Loss: 1.41547 Disc Loss: 0.585285\n",
      "Gen Loss: 1.58267 Disc Loss: 0.640476\n",
      "Gen Loss: 2.49678 Disc Loss: 0.546662\n",
      "Gen Loss: 2.36307 Disc Loss: 0.389566\n",
      "Gen Loss: 1.06065 Disc Loss: 0.785644\n",
      "Gen Loss: 2.23573 Disc Loss: 0.471906\n",
      "Gen Loss: 2.19558 Disc Loss: 0.274737\n",
      "Gen Loss: 1.86028 Disc Loss: 0.565239\n",
      "Gen Loss: 3.51731 Disc Loss: 0.203549\n",
      "Gen Loss: 1.86777 Disc Loss: 0.501457\n",
      "Gen Loss: 2.13362 Disc Loss: 0.488661\n",
      "Gen Loss: 2.09885 Disc Loss: 0.479172\n",
      "Gen Loss: 1.83503 Disc Loss: 0.868208\n",
      "Gen Loss: 2.15745 Disc Loss: 0.400133\n",
      "Gen Loss: 2.25856 Disc Loss: 0.686516\n",
      "Gen Loss: 2.67895 Disc Loss: 0.741411\n",
      "Gen Loss: 1.58256 Disc Loss: 0.893062\n",
      "Gen Loss: 0.925472 Disc Loss: 1.26825\n",
      "Gen Loss: 2.03507 Disc Loss: 0.499034\n",
      "Gen Loss: 1.56853 Disc Loss: 0.679914\n",
      "Gen Loss: 1.99184 Disc Loss: 0.335472\n",
      "Gen Loss: 2.22505 Disc Loss: 0.477005\n",
      "Gen Loss: 2.36143 Disc Loss: 0.436331\n",
      "Gen Loss: 2.3323 Disc Loss: 0.290317\n",
      "Gen Loss: 3.0912 Disc Loss: 0.61285\n",
      "Gen Loss: 2.19015 Disc Loss: 0.533185\n",
      "Gen Loss: 2.38965 Disc Loss: 0.302355\n",
      "Gen Loss: 1.42798 Disc Loss: 0.72694\n",
      "Gen Loss: 2.7612 Disc Loss: 0.290827\n",
      "Gen Loss: 1.57858 Disc Loss: 0.545602\n",
      "Gen Loss: 2.21786 Disc Loss: 0.468543\n",
      "Gen Loss: 1.99126 Disc Loss: 0.396398\n",
      "Gen Loss: 2.18008 Disc Loss: 0.383798\n",
      "Gen Loss: 2.83799 Disc Loss: 0.661412\n",
      "Gen Loss: 1.88855 Disc Loss: 0.459989\n",
      "Gen Loss: 2.11852 Disc Loss: 0.366315\n",
      "Gen Loss: 1.00698 Disc Loss: 0.743994\n",
      "Gen Loss: 1.43142 Disc Loss: 0.714625\n",
      "Gen Loss: 1.70572 Disc Loss: 0.612676\n",
      "Gen Loss: 1.78623 Disc Loss: 0.600736\n",
      "Gen Loss: 2.27163 Disc Loss: 0.52398\n",
      "Gen Loss: 1.80178 Disc Loss: 0.599642\n",
      "Gen Loss: 2.52492 Disc Loss: 0.547308\n",
      "Gen Loss: 1.64414 Disc Loss: 0.481927\n",
      "Gen Loss: 1.83973 Disc Loss: 0.556896\n",
      "Gen Loss: 2.15649 Disc Loss: 0.340609\n",
      "Gen Loss: 1.30736 Disc Loss: 0.571526\n",
      "Gen Loss: 1.78064 Disc Loss: 0.468375\n",
      "Gen Loss: 1.82709 Disc Loss: 0.573383\n",
      "Gen Loss: 1.56802 Disc Loss: 0.673131\n",
      "Gen Loss: 1.80521 Disc Loss: 0.5931\n",
      "Gen Loss: 1.31055 Disc Loss: 0.609761\n",
      "Gen Loss: 1.00201 Disc Loss: 0.869265\n",
      "Gen Loss: 1.59385 Disc Loss: 0.562812\n",
      "Gen Loss: 1.21556 Disc Loss: 1.00057\n",
      "Gen Loss: 1.92548 Disc Loss: 0.44857\n",
      "Gen Loss: 2.02313 Disc Loss: 0.676981\n",
      "Gen Loss: 1.33334 Disc Loss: 0.856989\n",
      "Saved Model\n",
      "Gen Loss: 2.12028 Disc Loss: 0.50486\n",
      "Gen Loss: 1.66125 Disc Loss: 0.560819\n",
      "Gen Loss: 2.16757 Disc Loss: 0.532028\n",
      "Gen Loss: 2.05419 Disc Loss: 0.551551\n",
      "Gen Loss: 1.52524 Disc Loss: 0.385932\n",
      "Gen Loss: 1.29851 Disc Loss: 0.434935\n",
      "Gen Loss: 2.03998 Disc Loss: 0.430644\n",
      "Gen Loss: 1.86033 Disc Loss: 0.426317\n",
      "Gen Loss: 2.18107 Disc Loss: 0.436893\n",
      "Gen Loss: 1.26781 Disc Loss: 0.817567\n",
      "Gen Loss: 2.05656 Disc Loss: 0.377623\n",
      "Gen Loss: 2.14673 Disc Loss: 0.594084\n",
      "Gen Loss: 2.24416 Disc Loss: 0.460476\n",
      "Gen Loss: 2.27434 Disc Loss: 0.522494\n",
      "Gen Loss: 1.71889 Disc Loss: 0.448832\n",
      "Gen Loss: 1.80567 Disc Loss: 0.606919\n",
      "Gen Loss: 0.918171 Disc Loss: 0.814187\n",
      "Gen Loss: 0.937801 Disc Loss: 0.915248\n",
      "Gen Loss: 3.14339 Disc Loss: 0.718383\n",
      "Gen Loss: 1.12431 Disc Loss: 0.557902\n",
      "Gen Loss: 1.75376 Disc Loss: 0.545145\n",
      "Gen Loss: 1.80109 Disc Loss: 0.521022\n",
      "Gen Loss: 2.04415 Disc Loss: 0.402446\n",
      "Gen Loss: 1.35931 Disc Loss: 0.948079\n",
      "Gen Loss: 2.17968 Disc Loss: 0.759505\n",
      "Gen Loss: 2.31754 Disc Loss: 0.693996\n",
      "Gen Loss: 2.55803 Disc Loss: 0.551803\n",
      "Gen Loss: 1.00519 Disc Loss: 1.00764\n",
      "Gen Loss: 2.03359 Disc Loss: 0.340581\n",
      "Gen Loss: 2.00221 Disc Loss: 0.42355\n",
      "Gen Loss: 1.60551 Disc Loss: 0.5557\n",
      "Gen Loss: 1.34044 Disc Loss: 0.721789\n",
      "Gen Loss: 0.571213 Disc Loss: 1.13738\n",
      "Gen Loss: 1.84844 Disc Loss: 0.682817\n",
      "Gen Loss: 2.21964 Disc Loss: 0.644313\n",
      "Gen Loss: 1.49778 Disc Loss: 0.959416\n",
      "Gen Loss: 3.03402 Disc Loss: 0.483165\n",
      "Gen Loss: 1.102 Disc Loss: 0.555847\n",
      "Gen Loss: 1.90671 Disc Loss: 0.56147\n",
      "Gen Loss: 1.52128 Disc Loss: 0.478324\n",
      "Gen Loss: 2.20246 Disc Loss: 0.449013\n",
      "Gen Loss: 3.26234 Disc Loss: 0.372313\n",
      "Gen Loss: 3.49858 Disc Loss: 0.835825\n",
      "Gen Loss: 1.49141 Disc Loss: 0.416901\n",
      "Gen Loss: 1.75902 Disc Loss: 1.15784\n",
      "Gen Loss: 1.83191 Disc Loss: 0.675925\n",
      "Gen Loss: 2.11429 Disc Loss: 0.333159\n",
      "Gen Loss: 2.52237 Disc Loss: 0.465958\n",
      "Gen Loss: 2.14673 Disc Loss: 0.291052\n",
      "Gen Loss: 1.57077 Disc Loss: 1.21447\n",
      "Gen Loss: 2.68922 Disc Loss: 0.339199\n",
      "Gen Loss: 1.85232 Disc Loss: 0.373951\n",
      "Gen Loss: 1.58879 Disc Loss: 0.830484\n",
      "Gen Loss: 1.9111 Disc Loss: 0.654926\n",
      "Gen Loss: 2.0557 Disc Loss: 0.409858\n",
      "Gen Loss: 2.18112 Disc Loss: 0.32534\n",
      "Gen Loss: 1.87747 Disc Loss: 0.330975\n",
      "Gen Loss: 1.7031 Disc Loss: 0.472031\n",
      "Gen Loss: 1.64285 Disc Loss: 0.64003\n",
      "Gen Loss: 1.90616 Disc Loss: 0.518195\n",
      "Gen Loss: 1.98257 Disc Loss: 0.496028\n",
      "Gen Loss: 2.02305 Disc Loss: 0.591546\n",
      "Gen Loss: 2.49708 Disc Loss: 0.387379\n",
      "Gen Loss: 0.971191 Disc Loss: 0.474879\n",
      "Gen Loss: 2.3893 Disc Loss: 0.41583\n",
      "Gen Loss: 1.65258 Disc Loss: 0.541705\n",
      "Gen Loss: 1.30499 Disc Loss: 0.638438\n",
      "Gen Loss: 2.16009 Disc Loss: 0.353345\n",
      "Gen Loss: 2.05274 Disc Loss: 0.415738\n",
      "Gen Loss: 1.85813 Disc Loss: 0.424024\n",
      "Gen Loss: 1.70418 Disc Loss: 0.507176\n",
      "Gen Loss: 1.91729 Disc Loss: 0.558177\n",
      "Gen Loss: 1.55873 Disc Loss: 0.480477\n",
      "Gen Loss: 1.65837 Disc Loss: 1.20435\n",
      "Gen Loss: 1.34103 Disc Loss: 0.737614\n",
      "Gen Loss: 1.12318 Disc Loss: 0.837492\n",
      "Gen Loss: 2.64701 Disc Loss: 0.898336\n",
      "Gen Loss: 1.73731 Disc Loss: 0.596834\n",
      "Gen Loss: 1.9368 Disc Loss: 0.377207\n",
      "Gen Loss: 1.7925 Disc Loss: 0.602735\n",
      "Gen Loss: 2.27905 Disc Loss: 0.497457\n",
      "Gen Loss: 1.49557 Disc Loss: 0.524305\n",
      "Gen Loss: 2.08676 Disc Loss: 0.628826\n",
      "Gen Loss: 0.592551 Disc Loss: 1.26771\n",
      "Gen Loss: 1.44239 Disc Loss: 0.54099\n",
      "Gen Loss: 1.78469 Disc Loss: 0.707662\n",
      "Gen Loss: 1.4527 Disc Loss: 0.567945\n",
      "Gen Loss: 1.58719 Disc Loss: 0.457877\n",
      "Gen Loss: 2.28461 Disc Loss: 0.612481\n",
      "Gen Loss: 1.94446 Disc Loss: 0.373254\n",
      "Gen Loss: 1.97283 Disc Loss: 0.464128\n",
      "Gen Loss: 1.39539 Disc Loss: 0.706986\n",
      "Gen Loss: 1.09174 Disc Loss: 0.752115\n",
      "Gen Loss: 1.34905 Disc Loss: 0.688922\n",
      "Gen Loss: 1.87811 Disc Loss: 0.387239\n",
      "Gen Loss: 1.68927 Disc Loss: 0.478651\n",
      "Gen Loss: 1.01644 Disc Loss: 0.804829\n",
      "Gen Loss: 0.817492 Disc Loss: 1.16091\n",
      "Gen Loss: 1.89872 Disc Loss: 0.522662\n",
      "Gen Loss: 1.86776 Disc Loss: 0.510316\n",
      "Saved Model\n",
      "Gen Loss: 2.32318 Disc Loss: 0.723134\n",
      "Gen Loss: 1.15325 Disc Loss: 0.775055\n",
      "Gen Loss: 0.955851 Disc Loss: 0.643088\n",
      "Gen Loss: 1.69418 Disc Loss: 1.1218\n",
      "Gen Loss: 1.71765 Disc Loss: 0.431028\n",
      "Gen Loss: 0.834211 Disc Loss: 0.736837\n",
      "Gen Loss: 1.59207 Disc Loss: 0.729965\n",
      "Gen Loss: 1.95219 Disc Loss: 0.613283\n",
      "Gen Loss: 1.54362 Disc Loss: 0.676758\n",
      "Gen Loss: 2.16874 Disc Loss: 0.436467\n",
      "Gen Loss: 2.12703 Disc Loss: 0.604009\n",
      "Gen Loss: 1.83403 Disc Loss: 0.591899\n",
      "Gen Loss: 1.77767 Disc Loss: 0.65814\n",
      "Gen Loss: 1.39304 Disc Loss: 0.594708\n",
      "Gen Loss: 0.958881 Disc Loss: 0.575953\n",
      "Gen Loss: 2.25153 Disc Loss: 0.518893\n",
      "Gen Loss: 2.49643 Disc Loss: 0.215603\n",
      "Gen Loss: 1.65002 Disc Loss: 0.486168\n",
      "Gen Loss: 2.02813 Disc Loss: 0.259688\n",
      "Gen Loss: 2.2359 Disc Loss: 0.222098\n",
      "Gen Loss: 2.30207 Disc Loss: 0.285705\n",
      "Gen Loss: 1.99878 Disc Loss: 0.360755\n",
      "Gen Loss: 2.2618 Disc Loss: 0.553391\n",
      "Gen Loss: 2.00185 Disc Loss: 0.457256\n",
      "Gen Loss: 1.33721 Disc Loss: 0.51425\n",
      "Gen Loss: 0.924877 Disc Loss: 0.986578\n",
      "Gen Loss: 2.22118 Disc Loss: 0.676153\n",
      "Gen Loss: 1.62731 Disc Loss: 0.5317\n",
      "Gen Loss: 2.16734 Disc Loss: 0.481183\n",
      "Gen Loss: 2.83998 Disc Loss: 0.400049\n",
      "Gen Loss: 1.59808 Disc Loss: 0.391129\n",
      "Gen Loss: 1.56609 Disc Loss: 0.617402\n",
      "Gen Loss: 1.78086 Disc Loss: 0.52492\n",
      "Gen Loss: 2.49991 Disc Loss: 0.403083\n",
      "Gen Loss: 2.39999 Disc Loss: 0.371199\n",
      "Gen Loss: 2.08839 Disc Loss: 0.426626\n",
      "Gen Loss: 2.33393 Disc Loss: 0.521693\n",
      "Gen Loss: 2.34959 Disc Loss: 0.497982\n",
      "Gen Loss: 2.01068 Disc Loss: 0.307492\n",
      "Gen Loss: 1.79411 Disc Loss: 0.508224\n",
      "Gen Loss: 2.87072 Disc Loss: 0.475123\n",
      "Gen Loss: 1.27377 Disc Loss: 0.708629\n",
      "Gen Loss: 1.47673 Disc Loss: 0.825379\n",
      "Gen Loss: 1.97813 Disc Loss: 0.487684\n",
      "Gen Loss: 1.90834 Disc Loss: 0.507624\n",
      "Gen Loss: 1.83628 Disc Loss: 0.701694\n",
      "Gen Loss: 2.35509 Disc Loss: 0.651621\n",
      "Gen Loss: 1.92437 Disc Loss: 0.415016\n",
      "Gen Loss: 2.46334 Disc Loss: 0.551846\n",
      "Gen Loss: 1.44707 Disc Loss: 0.688286\n",
      "Gen Loss: 1.44363 Disc Loss: 0.655759\n",
      "Gen Loss: 0.960878 Disc Loss: 1.6079\n",
      "Gen Loss: 1.64524 Disc Loss: 0.639025\n",
      "Gen Loss: 2.18212 Disc Loss: 0.718498\n",
      "Gen Loss: 1.12132 Disc Loss: 0.788509\n",
      "Gen Loss: 2.87788 Disc Loss: 0.338454\n",
      "Gen Loss: 1.05021 Disc Loss: 0.44112\n",
      "Gen Loss: 1.07839 Disc Loss: 0.620823\n",
      "Gen Loss: 1.72885 Disc Loss: 0.769826\n",
      "Gen Loss: 1.90101 Disc Loss: 0.474389\n",
      "Gen Loss: 0.902932 Disc Loss: 0.811563\n",
      "Gen Loss: 2.12146 Disc Loss: 0.347387\n",
      "Gen Loss: 0.918484 Disc Loss: 0.663496\n",
      "Gen Loss: 1.44056 Disc Loss: 0.623148\n",
      "Gen Loss: 1.99582 Disc Loss: 0.439173\n",
      "Gen Loss: 1.6872 Disc Loss: 0.329361\n",
      "Gen Loss: 1.70605 Disc Loss: 0.461087\n",
      "Gen Loss: 2.01863 Disc Loss: 0.415116\n",
      "Gen Loss: 1.5186 Disc Loss: 0.432377\n",
      "Gen Loss: 2.5113 Disc Loss: 1.30151\n",
      "Gen Loss: 1.94593 Disc Loss: 0.546672\n",
      "Gen Loss: 0.812913 Disc Loss: 1.02034\n",
      "Gen Loss: 1.56196 Disc Loss: 0.695119\n",
      "Gen Loss: 1.82253 Disc Loss: 0.451336\n",
      "Gen Loss: 1.61276 Disc Loss: 0.704165\n",
      "Gen Loss: 1.72569 Disc Loss: 0.710138\n",
      "Gen Loss: 1.02997 Disc Loss: 0.82013\n",
      "Gen Loss: 1.63908 Disc Loss: 0.665298\n",
      "Gen Loss: 2.07488 Disc Loss: 0.443883\n",
      "Gen Loss: 3.40984 Disc Loss: 0.753498\n",
      "Gen Loss: 1.97589 Disc Loss: 0.840259\n",
      "Gen Loss: 1.5108 Disc Loss: 0.794738\n",
      "Gen Loss: 1.95729 Disc Loss: 0.987687\n",
      "Gen Loss: 1.62107 Disc Loss: 0.53652\n",
      "Gen Loss: 1.70795 Disc Loss: 0.788304\n",
      "Gen Loss: 1.36891 Disc Loss: 0.550734\n",
      "Gen Loss: 1.42978 Disc Loss: 0.609139\n",
      "Gen Loss: 1.56894 Disc Loss: 0.594474\n",
      "Gen Loss: 2.2554 Disc Loss: 0.534113\n",
      "Gen Loss: 2.55369 Disc Loss: 0.596271\n",
      "Gen Loss: 3.49938 Disc Loss: 1.22111\n",
      "Gen Loss: 1.69607 Disc Loss: 0.496997\n",
      "Gen Loss: 1.98665 Disc Loss: 0.472392\n",
      "Gen Loss: 1.05236 Disc Loss: 0.514541\n",
      "Gen Loss: 2.68603 Disc Loss: 0.447515\n",
      "Gen Loss: 2.10352 Disc Loss: 0.400156\n",
      "Gen Loss: 2.48432 Disc Loss: 0.311768\n",
      "Gen Loss: 2.14018 Disc Loss: 0.355108\n",
      "Gen Loss: 2.66145 Disc Loss: 0.457199\n",
      "Gen Loss: 3.06951 Disc Loss: 0.653739\n",
      "Saved Model\n",
      "Gen Loss: 2.18726 Disc Loss: 0.529849\n",
      "Gen Loss: 2.54737 Disc Loss: 0.555704\n",
      "Gen Loss: 2.64125 Disc Loss: 0.695758\n",
      "Gen Loss: 4.00829 Disc Loss: 0.400676\n",
      "Gen Loss: 2.83811 Disc Loss: 0.238126\n",
      "Gen Loss: 2.85209 Disc Loss: 0.23344\n",
      "Gen Loss: 2.40538 Disc Loss: 0.599395\n",
      "Gen Loss: 2.48022 Disc Loss: 0.300774\n",
      "Gen Loss: 2.57859 Disc Loss: 0.26641\n",
      "Gen Loss: 2.8017 Disc Loss: 0.214224\n",
      "Gen Loss: 2.22727 Disc Loss: 0.279929\n",
      "Gen Loss: 1.75629 Disc Loss: 0.418512\n",
      "Gen Loss: 1.93702 Disc Loss: 0.489925\n",
      "Gen Loss: 2.91344 Disc Loss: 0.715364\n",
      "Gen Loss: 1.89269 Disc Loss: 0.524657\n",
      "Gen Loss: 1.5974 Disc Loss: 0.349295\n",
      "Gen Loss: 1.60843 Disc Loss: 0.479894\n",
      "Gen Loss: 1.93337 Disc Loss: 0.373288\n",
      "Gen Loss: 1.56677 Disc Loss: 0.51764\n",
      "Gen Loss: 2.34077 Disc Loss: 0.549458\n",
      "Gen Loss: 3.38524 Disc Loss: 0.610243\n",
      "Gen Loss: 2.11077 Disc Loss: 0.383054\n",
      "Gen Loss: 2.62488 Disc Loss: 0.554972\n",
      "Gen Loss: 3.3062 Disc Loss: 0.621763\n",
      "Gen Loss: 2.44515 Disc Loss: 0.767551\n",
      "Gen Loss: 1.48451 Disc Loss: 0.455577\n",
      "Gen Loss: 2.72213 Disc Loss: 0.734168\n",
      "Gen Loss: 2.0856 Disc Loss: 0.448254\n",
      "Gen Loss: 1.6411 Disc Loss: 0.708963\n",
      "Gen Loss: 2.85037 Disc Loss: 0.576115\n",
      "Gen Loss: 2.73442 Disc Loss: 0.329705\n",
      "Gen Loss: 1.70909 Disc Loss: 0.44375\n",
      "Gen Loss: 2.30222 Disc Loss: 0.289112\n",
      "Gen Loss: 1.55081 Disc Loss: 0.611503\n",
      "Gen Loss: 2.27836 Disc Loss: 0.36382\n",
      "Gen Loss: 2.14383 Disc Loss: 0.263488\n",
      "Gen Loss: 1.70649 Disc Loss: 0.434685\n",
      "Gen Loss: 1.8497 Disc Loss: 0.474959\n",
      "Gen Loss: 2.00397 Disc Loss: 0.303391\n",
      "Gen Loss: 2.20652 Disc Loss: 0.519729\n",
      "Gen Loss: 1.29234 Disc Loss: 0.625925\n",
      "Gen Loss: 0.972279 Disc Loss: 0.925657\n",
      "Gen Loss: 0.831167 Disc Loss: 0.700041\n",
      "Gen Loss: 1.97144 Disc Loss: 0.388616\n",
      "Gen Loss: 1.49391 Disc Loss: 0.734528\n",
      "Gen Loss: 1.08593 Disc Loss: 0.576158\n",
      "Gen Loss: 2.42838 Disc Loss: 0.602304\n",
      "Gen Loss: 2.35265 Disc Loss: 0.465115\n",
      "Gen Loss: 2.02403 Disc Loss: 0.560701\n",
      "Gen Loss: 2.22755 Disc Loss: 0.407867\n",
      "Gen Loss: 2.77521 Disc Loss: 0.307477\n",
      "Gen Loss: 2.12783 Disc Loss: 0.306753\n",
      "Gen Loss: 2.8571 Disc Loss: 0.2658\n",
      "Gen Loss: 2.33937 Disc Loss: 0.425683\n",
      "Gen Loss: 2.03132 Disc Loss: 0.432961\n",
      "Gen Loss: 1.93059 Disc Loss: 0.342375\n",
      "Gen Loss: 2.87918 Disc Loss: 0.70144\n",
      "Gen Loss: 2.13621 Disc Loss: 0.381802\n",
      "Gen Loss: 2.22589 Disc Loss: 0.382032\n",
      "Gen Loss: 2.41829 Disc Loss: 0.256104\n",
      "Gen Loss: 2.67943 Disc Loss: 0.295604\n",
      "Gen Loss: 2.4531 Disc Loss: 0.562366\n",
      "Gen Loss: 2.41837 Disc Loss: 0.557117\n",
      "Gen Loss: 2.19513 Disc Loss: 0.922639\n",
      "Gen Loss: 3.0347 Disc Loss: 0.473036\n",
      "Gen Loss: 2.70412 Disc Loss: 0.333339\n",
      "Gen Loss: 2.31967 Disc Loss: 0.368633\n",
      "Gen Loss: 2.15203 Disc Loss: 0.351733\n",
      "Gen Loss: 2.14178 Disc Loss: 0.484872\n",
      "Gen Loss: 2.08817 Disc Loss: 0.319843\n",
      "Gen Loss: 2.42373 Disc Loss: 0.510541\n",
      "Gen Loss: 3.00233 Disc Loss: 0.382185\n",
      "Gen Loss: 2.3931 Disc Loss: 0.303655\n",
      "Gen Loss: 1.4352 Disc Loss: 0.480871\n",
      "Gen Loss: 3.21219 Disc Loss: 0.426806\n",
      "Gen Loss: 2.02028 Disc Loss: 0.427871\n",
      "Gen Loss: 2.7513 Disc Loss: 0.280256\n",
      "Gen Loss: 1.43955 Disc Loss: 1.28112\n",
      "Gen Loss: 1.64831 Disc Loss: 0.717842\n",
      "Gen Loss: 2.37137 Disc Loss: 0.452097\n",
      "Gen Loss: 2.00826 Disc Loss: 0.546474\n",
      "Gen Loss: 2.45799 Disc Loss: 0.509916\n",
      "Gen Loss: 2.24001 Disc Loss: 0.317867\n",
      "Gen Loss: 2.45702 Disc Loss: 0.307927\n",
      "Gen Loss: 2.12937 Disc Loss: 0.358707\n",
      "Gen Loss: 1.06835 Disc Loss: 0.983977\n",
      "Gen Loss: 3.01997 Disc Loss: 0.433283\n",
      "Gen Loss: 1.98672 Disc Loss: 0.571308\n",
      "Gen Loss: 2.58858 Disc Loss: 0.46836\n",
      "Gen Loss: 4.00412 Disc Loss: 0.65433\n",
      "Gen Loss: 2.75617 Disc Loss: 0.314805\n",
      "Gen Loss: 3.34177 Disc Loss: 0.343962\n",
      "Gen Loss: 2.11257 Disc Loss: 0.399608\n",
      "Gen Loss: 1.58308 Disc Loss: 0.553766\n",
      "Gen Loss: 2.55316 Disc Loss: 0.704603\n",
      "Gen Loss: 2.79917 Disc Loss: 0.473206\n",
      "Gen Loss: 1.29081 Disc Loss: 0.683088\n",
      "Gen Loss: 1.92284 Disc Loss: 0.382253\n",
      "Gen Loss: 2.30296 Disc Loss: 0.408382\n",
      "Gen Loss: 2.6895 Disc Loss: 0.505794\n",
      "Saved Model\n",
      "Gen Loss: 3.77628 Disc Loss: 0.527362\n",
      "Gen Loss: 2.5453 Disc Loss: 0.455421\n",
      "Gen Loss: 2.73163 Disc Loss: 0.345118\n",
      "Gen Loss: 2.53852 Disc Loss: 0.357607\n",
      "Gen Loss: 1.76216 Disc Loss: 0.636475\n",
      "Gen Loss: 3.06207 Disc Loss: 0.733746\n",
      "Gen Loss: 1.94978 Disc Loss: 0.419054\n",
      "Gen Loss: 2.31695 Disc Loss: 0.285077\n",
      "Gen Loss: 2.00066 Disc Loss: 0.417601\n",
      "Gen Loss: 2.06967 Disc Loss: 0.440407\n",
      "Gen Loss: 1.93147 Disc Loss: 0.449306\n",
      "Gen Loss: 1.44795 Disc Loss: 0.550385\n",
      "Gen Loss: 1.07731 Disc Loss: 0.5191\n",
      "Gen Loss: 1.97928 Disc Loss: 0.443656\n",
      "Gen Loss: 1.5597 Disc Loss: 0.381891\n",
      "Gen Loss: 2.11179 Disc Loss: 0.466822\n",
      "Gen Loss: 1.75633 Disc Loss: 0.386542\n",
      "Gen Loss: 2.22905 Disc Loss: 0.297759\n",
      "Gen Loss: 2.78035 Disc Loss: 0.352577\n",
      "Gen Loss: 2.08028 Disc Loss: 0.266893\n",
      "Gen Loss: 1.45938 Disc Loss: 0.293674\n",
      "Gen Loss: 0.937936 Disc Loss: 0.644936\n",
      "Gen Loss: 1.99813 Disc Loss: 0.40683\n",
      "Gen Loss: 1.37274 Disc Loss: 0.655683\n",
      "Gen Loss: 1.23579 Disc Loss: 0.556067\n",
      "Gen Loss: 2.35827 Disc Loss: 0.442235\n",
      "Gen Loss: 2.39983 Disc Loss: 0.422935\n",
      "Gen Loss: 1.9281 Disc Loss: 0.43652\n",
      "Gen Loss: 3.43758 Disc Loss: 0.293096\n",
      "Gen Loss: 2.92563 Disc Loss: 0.364802\n",
      "Gen Loss: 2.05886 Disc Loss: 0.609173\n",
      "Gen Loss: 2.10983 Disc Loss: 0.36289\n",
      "Gen Loss: 1.66619 Disc Loss: 0.309862\n",
      "Gen Loss: 2.41256 Disc Loss: 0.314731\n",
      "Gen Loss: 1.45167 Disc Loss: 0.668285\n",
      "Gen Loss: 2.10691 Disc Loss: 0.420921\n",
      "Gen Loss: 2.92155 Disc Loss: 0.490339\n",
      "Gen Loss: 1.72565 Disc Loss: 0.514782\n",
      "Gen Loss: 1.81542 Disc Loss: 0.376032\n",
      "Gen Loss: 1.53279 Disc Loss: 0.350987\n",
      "Gen Loss: 3.072 Disc Loss: 0.421978\n",
      "Gen Loss: 2.37333 Disc Loss: 0.446928\n",
      "Gen Loss: 2.51233 Disc Loss: 0.413895\n",
      "Gen Loss: 2.41351 Disc Loss: 0.284568\n",
      "Gen Loss: 2.11233 Disc Loss: 0.367757\n",
      "Gen Loss: 2.30307 Disc Loss: 0.424002\n",
      "Gen Loss: 2.15548 Disc Loss: 0.471128\n",
      "Gen Loss: 2.96476 Disc Loss: 0.63609\n",
      "Gen Loss: 1.02418 Disc Loss: 1.71367\n",
      "Gen Loss: 0.273457 Disc Loss: 1.24826\n",
      "Gen Loss: 2.3233 Disc Loss: 1.16073\n",
      "Gen Loss: 1.12763 Disc Loss: 0.75755\n",
      "Gen Loss: 1.27691 Disc Loss: 0.708824\n",
      "Gen Loss: 1.69703 Disc Loss: 0.66997\n",
      "Gen Loss: 1.39741 Disc Loss: 0.52243\n",
      "Gen Loss: 2.45323 Disc Loss: 0.626923\n",
      "Gen Loss: 1.01642 Disc Loss: 0.638132\n",
      "Gen Loss: 1.39283 Disc Loss: 0.677833\n",
      "Gen Loss: 2.50891 Disc Loss: 0.316499\n",
      "Gen Loss: 1.65788 Disc Loss: 0.901506\n",
      "Gen Loss: 1.23522 Disc Loss: 0.960337\n",
      "Gen Loss: 1.63978 Disc Loss: 0.465012\n",
      "Gen Loss: 2.1954 Disc Loss: 0.365777\n",
      "Gen Loss: 1.25407 Disc Loss: 0.508235\n",
      "Gen Loss: 2.38204 Disc Loss: 0.519371\n",
      "Gen Loss: 2.45818 Disc Loss: 0.745669\n",
      "Gen Loss: 1.542 Disc Loss: 0.583201\n",
      "Gen Loss: 2.4168 Disc Loss: 0.485775\n",
      "Gen Loss: 1.37989 Disc Loss: 0.328061\n",
      "Gen Loss: 2.37915 Disc Loss: 0.396383\n",
      "Gen Loss: 2.06402 Disc Loss: 0.327778\n",
      "Gen Loss: 1.74115 Disc Loss: 0.336635\n",
      "Gen Loss: 2.72398 Disc Loss: 0.30135\n",
      "Gen Loss: 2.55975 Disc Loss: 0.23714\n",
      "Gen Loss: 2.50482 Disc Loss: 0.242445\n",
      "Gen Loss: 1.73641 Disc Loss: 0.380121\n",
      "Gen Loss: 2.93698 Disc Loss: 0.423797\n",
      "Gen Loss: 2.84706 Disc Loss: 0.438421\n",
      "Gen Loss: 1.94772 Disc Loss: 0.37269\n",
      "Gen Loss: 2.26526 Disc Loss: 0.261172\n",
      "Gen Loss: 1.69586 Disc Loss: 0.397867\n",
      "Gen Loss: 2.13747 Disc Loss: 0.612174\n",
      "Gen Loss: 1.99069 Disc Loss: 0.440043\n",
      "Gen Loss: 1.62569 Disc Loss: 0.440058\n",
      "Gen Loss: 2.91397 Disc Loss: 0.682628\n",
      "Gen Loss: 0.86971 Disc Loss: 0.768927\n",
      "Gen Loss: 0.699514 Disc Loss: 0.847274\n",
      "Gen Loss: 1.80781 Disc Loss: 0.66532\n",
      "Gen Loss: 1.75799 Disc Loss: 0.488859\n",
      "Gen Loss: 1.20575 Disc Loss: 0.772851\n",
      "Gen Loss: 1.98665 Disc Loss: 0.470538\n",
      "Gen Loss: 2.53388 Disc Loss: 0.434553\n",
      "Gen Loss: 2.26207 Disc Loss: 0.317515\n",
      "Gen Loss: 0.809031 Disc Loss: 1.00349\n",
      "Gen Loss: 2.04091 Disc Loss: 0.583027\n",
      "Gen Loss: 3.65222 Disc Loss: 0.69647\n",
      "Gen Loss: 2.78308 Disc Loss: 0.290312\n",
      "Gen Loss: 0.616168 Disc Loss: 1.00994\n",
      "Gen Loss: 1.53465 Disc Loss: 0.559295\n",
      "Gen Loss: 3.09401 Disc Loss: 0.594432\n",
      "Saved Model\n",
      "Gen Loss: 2.53476 Disc Loss: 0.337739\n",
      "Gen Loss: 2.54749 Disc Loss: 0.320758\n",
      "Gen Loss: 1.31587 Disc Loss: 0.709518\n",
      "Gen Loss: 2.21893 Disc Loss: 0.585779\n",
      "Gen Loss: 1.64674 Disc Loss: 0.760908\n",
      "Gen Loss: 1.84919 Disc Loss: 0.507394\n",
      "Gen Loss: 2.09879 Disc Loss: 0.517518\n",
      "Gen Loss: 1.14259 Disc Loss: 0.900581\n",
      "Gen Loss: 2.81924 Disc Loss: 0.52938\n",
      "Gen Loss: 1.86739 Disc Loss: 0.402575\n",
      "Gen Loss: 2.15637 Disc Loss: 0.364846\n",
      "Gen Loss: 3.20228 Disc Loss: 0.687077\n",
      "Gen Loss: 2.303 Disc Loss: 0.517386\n",
      "Gen Loss: 2.78029 Disc Loss: 0.363157\n",
      "Gen Loss: 1.9862 Disc Loss: 0.369356\n",
      "Gen Loss: 1.43514 Disc Loss: 0.342356\n",
      "Gen Loss: 1.84639 Disc Loss: 0.426221\n",
      "Gen Loss: 2.27633 Disc Loss: 0.301689\n",
      "Gen Loss: 3.48525 Disc Loss: 0.239899\n",
      "Gen Loss: 1.87969 Disc Loss: 0.322192\n",
      "Gen Loss: 1.94834 Disc Loss: 0.402633\n",
      "Gen Loss: 2.7125 Disc Loss: 0.387636\n",
      "Gen Loss: 2.92784 Disc Loss: 0.503578\n",
      "Gen Loss: 1.69107 Disc Loss: 0.320039\n",
      "Gen Loss: 2.60889 Disc Loss: 0.302161\n",
      "Gen Loss: 3.91421 Disc Loss: 0.21389\n",
      "Gen Loss: 3.08569 Disc Loss: 0.541363\n",
      "Gen Loss: 2.61974 Disc Loss: 0.343762\n",
      "Gen Loss: 3.57813 Disc Loss: 0.861604\n",
      "Gen Loss: 2.31816 Disc Loss: 0.533168\n",
      "Gen Loss: 2.06902 Disc Loss: 0.563652\n",
      "Gen Loss: 1.11557 Disc Loss: 0.532848\n",
      "Gen Loss: 1.48735 Disc Loss: 0.563671\n",
      "Gen Loss: 0.57671 Disc Loss: 0.683718\n",
      "Gen Loss: 2.16777 Disc Loss: 0.551159\n",
      "Gen Loss: 3.52463 Disc Loss: 0.983652\n",
      "Gen Loss: 1.78023 Disc Loss: 0.566037\n",
      "Gen Loss: 2.78185 Disc Loss: 0.330115\n",
      "Gen Loss: 2.74509 Disc Loss: 0.651785\n",
      "Gen Loss: 1.91611 Disc Loss: 0.546285\n",
      "Gen Loss: 3.11936 Disc Loss: 0.752019\n",
      "Gen Loss: 1.98664 Disc Loss: 0.413485\n",
      "Gen Loss: 2.01224 Disc Loss: 0.383283\n",
      "Gen Loss: 2.17991 Disc Loss: 0.47334\n",
      "Gen Loss: 1.60813 Disc Loss: 0.374003\n",
      "Gen Loss: 1.93391 Disc Loss: 0.315899\n",
      "Gen Loss: 1.7513 Disc Loss: 0.527061\n",
      "Gen Loss: 1.54874 Disc Loss: 0.563422\n",
      "Gen Loss: 1.51612 Disc Loss: 0.568962\n",
      "Gen Loss: 1.29298 Disc Loss: 0.475048\n",
      "Gen Loss: 1.84103 Disc Loss: 0.30282\n",
      "Gen Loss: 2.38491 Disc Loss: 0.408547\n",
      "Gen Loss: 2.12544 Disc Loss: 0.333001\n",
      "Gen Loss: 2.28091 Disc Loss: 0.373796\n",
      "Gen Loss: 2.38072 Disc Loss: 0.500083\n",
      "Gen Loss: 2.68438 Disc Loss: 0.252702\n",
      "Gen Loss: 1.45009 Disc Loss: 0.477752\n",
      "Gen Loss: 1.16551 Disc Loss: 0.737009\n",
      "Gen Loss: 1.37901 Disc Loss: 0.583164\n",
      "Gen Loss: 1.25672 Disc Loss: 0.530839\n",
      "Gen Loss: 1.42822 Disc Loss: 0.617494\n",
      "Gen Loss: 2.17446 Disc Loss: 0.507206\n",
      "Gen Loss: 1.97685 Disc Loss: 0.494167\n",
      "Gen Loss: 1.31155 Disc Loss: 0.735301\n",
      "Gen Loss: 1.63297 Disc Loss: 0.563338\n",
      "Gen Loss: 2.56068 Disc Loss: 0.590738\n",
      "Gen Loss: 2.02344 Disc Loss: 0.804663\n",
      "Gen Loss: 2.5977 Disc Loss: 0.426032\n",
      "Gen Loss: 2.18578 Disc Loss: 0.413076\n",
      "Gen Loss: 1.45246 Disc Loss: 0.53787\n",
      "Gen Loss: 3.01945 Disc Loss: 0.628724\n",
      "Gen Loss: 1.32602 Disc Loss: 0.86433\n",
      "Gen Loss: 1.38989 Disc Loss: 0.53724\n",
      "Gen Loss: 2.05868 Disc Loss: 0.750328\n",
      "Gen Loss: 1.72591 Disc Loss: 0.645286\n",
      "Gen Loss: 1.88941 Disc Loss: 0.521654\n",
      "Gen Loss: 1.56657 Disc Loss: 0.841588\n",
      "Gen Loss: 2.33773 Disc Loss: 0.365792\n",
      "Gen Loss: 2.40793 Disc Loss: 0.736043\n",
      "Gen Loss: 1.02821 Disc Loss: 0.754178\n",
      "Gen Loss: 1.90278 Disc Loss: 0.402569\n",
      "Gen Loss: 1.92546 Disc Loss: 0.36999\n",
      "Gen Loss: 1.40202 Disc Loss: 0.426215\n",
      "Gen Loss: 2.48182 Disc Loss: 0.325958\n",
      "Gen Loss: 2.54397 Disc Loss: 0.384197\n",
      "Gen Loss: 2.31612 Disc Loss: 0.312469\n",
      "Gen Loss: 2.21707 Disc Loss: 0.320575\n",
      "Gen Loss: 1.76526 Disc Loss: 0.556361\n",
      "Gen Loss: 2.09389 Disc Loss: 0.368552\n",
      "Gen Loss: 2.21403 Disc Loss: 0.331291\n",
      "Gen Loss: 1.44789 Disc Loss: 0.774311\n",
      "Gen Loss: 3.76441 Disc Loss: 0.690984\n",
      "Gen Loss: 1.45617 Disc Loss: 0.465945\n",
      "Gen Loss: 1.64304 Disc Loss: 0.295812\n",
      "Gen Loss: 1.5973 Disc Loss: 0.593342\n",
      "Gen Loss: 2.83109 Disc Loss: 0.43645\n",
      "Gen Loss: 2.54918 Disc Loss: 0.468707\n",
      "Gen Loss: 2.59451 Disc Loss: 0.534506\n",
      "Gen Loss: 1.95615 Disc Loss: 0.459389\n",
      "Gen Loss: 2.29973 Disc Loss: 0.470453\n",
      "Saved Model\n",
      "Gen Loss: 1.89168 Disc Loss: 0.450074\n",
      "Gen Loss: 2.52265 Disc Loss: 0.43774\n",
      "Gen Loss: 2.03151 Disc Loss: 0.246743\n",
      "Gen Loss: 3.47524 Disc Loss: 0.553016\n",
      "Gen Loss: 1.90114 Disc Loss: 0.523857\n",
      "Gen Loss: 2.69207 Disc Loss: 0.270642\n",
      "Gen Loss: 2.48478 Disc Loss: 0.399506\n",
      "Gen Loss: 2.16181 Disc Loss: 0.46803\n",
      "Gen Loss: 1.17215 Disc Loss: 0.651598\n",
      "Gen Loss: 1.53038 Disc Loss: 0.45489\n",
      "Gen Loss: 1.35837 Disc Loss: 0.54797\n",
      "Gen Loss: 1.26093 Disc Loss: 0.721775\n",
      "Gen Loss: 3.163 Disc Loss: 1.16279\n",
      "Gen Loss: 1.97436 Disc Loss: 0.44967\n",
      "Gen Loss: 1.68049 Disc Loss: 0.403777\n",
      "Gen Loss: 1.76244 Disc Loss: 0.712338\n",
      "Gen Loss: 1.21068 Disc Loss: 0.52809\n",
      "Gen Loss: 0.988172 Disc Loss: 0.616699\n",
      "Gen Loss: 0.722805 Disc Loss: 0.60822\n",
      "Gen Loss: 1.98868 Disc Loss: 0.538008\n",
      "Gen Loss: 1.61772 Disc Loss: 0.797164\n",
      "Gen Loss: 1.2982 Disc Loss: 0.53326\n",
      "Gen Loss: 1.04583 Disc Loss: 0.573874\n",
      "Gen Loss: 1.81676 Disc Loss: 0.494989\n",
      "Gen Loss: 1.57564 Disc Loss: 0.360982\n",
      "Gen Loss: 1.96595 Disc Loss: 0.238555\n",
      "Gen Loss: 1.48734 Disc Loss: 0.425354\n",
      "Gen Loss: 1.98335 Disc Loss: 0.772158\n",
      "Gen Loss: 2.1881 Disc Loss: 0.629963\n",
      "Gen Loss: 1.74175 Disc Loss: 0.540163\n",
      "Gen Loss: 1.89106 Disc Loss: 0.505916\n",
      "Gen Loss: 1.71514 Disc Loss: 0.602852\n",
      "Gen Loss: 2.6784 Disc Loss: 0.774422\n",
      "Gen Loss: 1.34544 Disc Loss: 0.718302\n",
      "Gen Loss: 1.18951 Disc Loss: 0.799594\n",
      "Gen Loss: 2.15793 Disc Loss: 0.62146\n",
      "Gen Loss: 2.43246 Disc Loss: 0.267498\n",
      "Gen Loss: 2.38374 Disc Loss: 0.34363\n",
      "Gen Loss: 2.29496 Disc Loss: 0.394313\n",
      "Gen Loss: 2.52247 Disc Loss: 0.331775\n",
      "Gen Loss: 3.07207 Disc Loss: 0.373764\n",
      "Gen Loss: 3.03819 Disc Loss: 0.396933\n",
      "Gen Loss: 1.70287 Disc Loss: 0.436502\n",
      "Gen Loss: 2.90173 Disc Loss: 0.560558\n",
      "Gen Loss: 2.91513 Disc Loss: 0.24203\n",
      "Gen Loss: 3.03511 Disc Loss: 0.371624\n",
      "Gen Loss: 2.9796 Disc Loss: 0.293194\n",
      "Gen Loss: 2.09697 Disc Loss: 0.423891\n",
      "Gen Loss: 3.38012 Disc Loss: 0.510033\n",
      "Gen Loss: 2.11578 Disc Loss: 0.314984\n",
      "Gen Loss: 2.68772 Disc Loss: 0.333838\n",
      "Gen Loss: 1.33072 Disc Loss: 0.306042\n",
      "Gen Loss: 2.30742 Disc Loss: 0.402118\n",
      "Gen Loss: 2.15927 Disc Loss: 0.268272\n",
      "Gen Loss: 2.40912 Disc Loss: 0.289896\n",
      "Gen Loss: 2.12424 Disc Loss: 0.500635\n",
      "Gen Loss: 2.47298 Disc Loss: 0.283485\n",
      "Gen Loss: 2.24939 Disc Loss: 0.290644\n",
      "Gen Loss: 1.75296 Disc Loss: 0.347466\n",
      "Gen Loss: 2.00251 Disc Loss: 0.493108\n",
      "Gen Loss: 2.16923 Disc Loss: 0.226605\n",
      "Gen Loss: 2.7918 Disc Loss: 0.724315\n",
      "Gen Loss: 2.27994 Disc Loss: 0.520783\n",
      "Gen Loss: 1.9747 Disc Loss: 0.639881\n",
      "Gen Loss: 2.77856 Disc Loss: 0.357943\n",
      "Gen Loss: 1.19735 Disc Loss: 0.835082\n",
      "Gen Loss: 1.93054 Disc Loss: 0.624236\n",
      "Gen Loss: 1.77553 Disc Loss: 0.531588\n",
      "Gen Loss: 2.51406 Disc Loss: 0.306121\n",
      "Gen Loss: 2.38289 Disc Loss: 0.360508\n",
      "Gen Loss: 2.27961 Disc Loss: 0.439154\n",
      "Gen Loss: 2.69738 Disc Loss: 0.434496\n",
      "Gen Loss: 2.37922 Disc Loss: 0.719568\n",
      "Gen Loss: 1.94114 Disc Loss: 0.310135\n",
      "Gen Loss: 3.03022 Disc Loss: 0.265096\n",
      "Gen Loss: 1.80533 Disc Loss: 0.437295\n",
      "Gen Loss: 2.69736 Disc Loss: 0.317132\n",
      "Gen Loss: 1.84726 Disc Loss: 0.331407\n",
      "Gen Loss: 1.86219 Disc Loss: 0.362566\n",
      "Gen Loss: 2.04208 Disc Loss: 0.41848\n",
      "Gen Loss: 2.00276 Disc Loss: 0.380507\n",
      "Gen Loss: 2.00037 Disc Loss: 0.442852\n",
      "Gen Loss: 2.11548 Disc Loss: 0.459009\n",
      "Gen Loss: 2.14361 Disc Loss: 0.406506\n",
      "Gen Loss: 2.38185 Disc Loss: 0.424753\n",
      "Gen Loss: 1.9788 Disc Loss: 0.556756\n",
      "Gen Loss: 2.18966 Disc Loss: 0.457181\n",
      "Gen Loss: 1.97256 Disc Loss: 0.533085\n",
      "Gen Loss: 2.28349 Disc Loss: 0.310993\n",
      "Gen Loss: 1.57357 Disc Loss: 0.49596\n",
      "Gen Loss: 1.98766 Disc Loss: 0.569106\n",
      "Gen Loss: 1.73595 Disc Loss: 0.464732\n",
      "Gen Loss: 1.84758 Disc Loss: 0.222025\n",
      "Gen Loss: 2.48117 Disc Loss: 0.504139\n",
      "Gen Loss: 1.93055 Disc Loss: 0.286301\n",
      "Gen Loss: 1.63465 Disc Loss: 0.474765\n",
      "Gen Loss: 2.62953 Disc Loss: 0.321443\n",
      "Gen Loss: 1.99825 Disc Loss: 0.402528\n",
      "Gen Loss: 2.97731 Disc Loss: 0.221527\n",
      "Gen Loss: 2.85521 Disc Loss: 0.212706\n",
      "Saved Model\n",
      "Gen Loss: 2.55056 Disc Loss: 0.404671\n",
      "Gen Loss: 2.55933 Disc Loss: 0.370967\n",
      "Gen Loss: 2.20757 Disc Loss: 0.40084\n",
      "Gen Loss: 5.90529 Disc Loss: 1.05693\n",
      "Gen Loss: 1.39682 Disc Loss: 0.688866\n",
      "Gen Loss: 3.29373 Disc Loss: 0.663914\n",
      "Gen Loss: 1.50101 Disc Loss: 0.476093\n",
      "Gen Loss: 1.81271 Disc Loss: 0.401423\n",
      "Gen Loss: 1.8035 Disc Loss: 0.551268\n",
      "Gen Loss: 1.49931 Disc Loss: 0.599358\n",
      "Gen Loss: 1.49554 Disc Loss: 0.603664\n",
      "Gen Loss: 1.82857 Disc Loss: 0.378982\n",
      "Gen Loss: 1.85476 Disc Loss: 0.550948\n",
      "Gen Loss: 2.27568 Disc Loss: 0.192999\n",
      "Gen Loss: 2.99901 Disc Loss: 0.495171\n",
      "Gen Loss: 2.86704 Disc Loss: 0.390548\n",
      "Gen Loss: 2.2287 Disc Loss: 0.344727\n",
      "Gen Loss: 2.55748 Disc Loss: 0.260337\n",
      "Gen Loss: 2.46922 Disc Loss: 0.281409\n",
      "Gen Loss: 2.41733 Disc Loss: 0.227805\n",
      "Gen Loss: 2.47924 Disc Loss: 0.274541\n",
      "Gen Loss: 2.13316 Disc Loss: 0.395583\n",
      "Gen Loss: 1.4046 Disc Loss: 0.586017\n",
      "Gen Loss: 2.82404 Disc Loss: 0.457254\n",
      "Gen Loss: 2.86571 Disc Loss: 0.492964\n",
      "Gen Loss: 1.86471 Disc Loss: 0.47964\n",
      "Gen Loss: 1.98685 Disc Loss: 0.455292\n",
      "Gen Loss: 2.20609 Disc Loss: 0.394262\n",
      "Gen Loss: 1.68801 Disc Loss: 0.471317\n",
      "Gen Loss: 2.68665 Disc Loss: 0.421462\n",
      "Gen Loss: 2.00305 Disc Loss: 0.576558\n",
      "Gen Loss: 1.45095 Disc Loss: 0.432501\n",
      "Gen Loss: 1.37115 Disc Loss: 0.54936\n",
      "Gen Loss: 1.07187 Disc Loss: 0.477693\n",
      "Gen Loss: 1.62961 Disc Loss: 0.539707\n",
      "Gen Loss: 1.34611 Disc Loss: 0.307051\n",
      "Gen Loss: 1.89794 Disc Loss: 0.449933\n",
      "Gen Loss: 1.52286 Disc Loss: 0.565401\n",
      "Gen Loss: 2.23551 Disc Loss: 0.484812\n",
      "Gen Loss: 1.93891 Disc Loss: 0.385897\n",
      "Gen Loss: 2.64008 Disc Loss: 0.564403\n",
      "Gen Loss: 4.07719 Disc Loss: 0.892888\n",
      "Gen Loss: 2.13751 Disc Loss: 0.399652\n",
      "Gen Loss: 2.25491 Disc Loss: 0.4475\n",
      "Gen Loss: 2.30007 Disc Loss: 0.375325\n",
      "Gen Loss: 1.98242 Disc Loss: 0.561669\n",
      "Gen Loss: 4.52703 Disc Loss: 0.83297\n",
      "Gen Loss: 3.68724 Disc Loss: 0.716133\n",
      "Gen Loss: 3.20047 Disc Loss: 0.720936\n",
      "Gen Loss: 1.27728 Disc Loss: 0.477995\n",
      "Gen Loss: 2.49089 Disc Loss: 0.294547\n",
      "Gen Loss: 1.77355 Disc Loss: 0.490712\n",
      "Gen Loss: 2.75273 Disc Loss: 0.259101\n",
      "Gen Loss: 1.85686 Disc Loss: 0.481743\n",
      "Gen Loss: 1.25094 Disc Loss: 0.486065\n",
      "Gen Loss: 0.782255 Disc Loss: 0.902814\n",
      "Gen Loss: 1.97572 Disc Loss: 0.481683\n",
      "Gen Loss: 2.06459 Disc Loss: 0.512763\n",
      "Gen Loss: 2.03708 Disc Loss: 0.473827\n",
      "Gen Loss: 1.72584 Disc Loss: 0.497762\n",
      "Gen Loss: 1.11167 Disc Loss: 0.618079\n",
      "Gen Loss: 1.60435 Disc Loss: 0.503175\n",
      "Gen Loss: 2.60524 Disc Loss: 0.691981\n",
      "Gen Loss: 1.9064 Disc Loss: 0.479947\n",
      "Gen Loss: 1.76376 Disc Loss: 0.616963\n",
      "Gen Loss: 1.36301 Disc Loss: 0.467337\n",
      "Gen Loss: 3.0293 Disc Loss: 0.609019\n",
      "Gen Loss: 2.26327 Disc Loss: 0.608073\n",
      "Gen Loss: 1.84805 Disc Loss: 0.464303\n",
      "Gen Loss: 0.669647 Disc Loss: 1.05227\n",
      "Gen Loss: 2.01701 Disc Loss: 0.564629\n",
      "Gen Loss: 1.67374 Disc Loss: 0.492948\n",
      "Gen Loss: 3.45805 Disc Loss: 0.792727\n",
      "Gen Loss: 2.10064 Disc Loss: 0.478198\n",
      "Gen Loss: 2.41974 Disc Loss: 0.708835\n",
      "Gen Loss: 3.02926 Disc Loss: 0.57002\n",
      "Gen Loss: 2.1578 Disc Loss: 0.723797\n",
      "Gen Loss: 1.01418 Disc Loss: 0.40251\n",
      "Gen Loss: 2.03869 Disc Loss: 0.281631\n",
      "Gen Loss: 2.63516 Disc Loss: 0.406037\n",
      "Gen Loss: 2.32993 Disc Loss: 0.537176\n",
      "Gen Loss: 1.807 Disc Loss: 0.34936\n",
      "Gen Loss: 1.44923 Disc Loss: 0.494223\n",
      "Gen Loss: 2.43981 Disc Loss: 0.389931\n",
      "Gen Loss: 2.92744 Disc Loss: 0.699042\n",
      "Gen Loss: 2.7765 Disc Loss: 0.416721\n",
      "Gen Loss: 2.02499 Disc Loss: 0.377641\n",
      "Gen Loss: 1.96032 Disc Loss: 0.226768\n",
      "Gen Loss: 2.39932 Disc Loss: 0.430628\n",
      "Gen Loss: 2.20954 Disc Loss: 0.472504\n",
      "Gen Loss: 1.53004 Disc Loss: 0.576475\n",
      "Gen Loss: 1.46772 Disc Loss: 0.715389\n",
      "Gen Loss: 2.95003 Disc Loss: 0.4944\n",
      "Gen Loss: 2.4806 Disc Loss: 0.34385\n",
      "Gen Loss: 2.70507 Disc Loss: 0.465971\n",
      "Gen Loss: 2.12113 Disc Loss: 0.580666\n",
      "Gen Loss: 1.80004 Disc Loss: 0.467267\n",
      "Gen Loss: 2.02318 Disc Loss: 0.439754\n",
      "Gen Loss: 2.50145 Disc Loss: 0.27085\n",
      "Gen Loss: 1.27056 Disc Loss: 0.575293\n",
      "Saved Model\n",
      "Gen Loss: 1.94403 Disc Loss: 0.5334\n",
      "Gen Loss: 2.06627 Disc Loss: 0.411944\n",
      "Gen Loss: 2.35826 Disc Loss: 0.292881\n",
      "Gen Loss: 2.18456 Disc Loss: 0.316407\n",
      "Gen Loss: 1.41552 Disc Loss: 0.368484\n",
      "Gen Loss: 2.76492 Disc Loss: 0.44199\n",
      "Gen Loss: 1.39531 Disc Loss: 0.312651\n",
      "Gen Loss: 0.953313 Disc Loss: 0.627005\n",
      "Gen Loss: 2.72288 Disc Loss: 0.246175\n",
      "Gen Loss: 2.35802 Disc Loss: 0.243007\n",
      "Gen Loss: 2.21295 Disc Loss: 0.433645\n",
      "Gen Loss: 2.2718 Disc Loss: 0.43846\n",
      "Gen Loss: 2.57069 Disc Loss: 0.363593\n",
      "Gen Loss: 2.38803 Disc Loss: 0.40824\n",
      "Gen Loss: 0.807505 Disc Loss: 0.720339\n",
      "Gen Loss: 2.53622 Disc Loss: 0.549229\n",
      "Gen Loss: 1.58889 Disc Loss: 0.457492\n",
      "Gen Loss: 1.72198 Disc Loss: 0.352444\n",
      "Gen Loss: 3.16364 Disc Loss: 0.239028\n",
      "Gen Loss: 2.08027 Disc Loss: 0.327492\n",
      "Gen Loss: 2.32992 Disc Loss: 0.325966\n",
      "Gen Loss: 2.6916 Disc Loss: 0.348786\n",
      "Gen Loss: 1.64683 Disc Loss: 0.555717\n",
      "Gen Loss: 2.76521 Disc Loss: 0.29056\n",
      "Gen Loss: 1.93648 Disc Loss: 0.345371\n",
      "Gen Loss: 2.03258 Disc Loss: 0.412375\n",
      "Gen Loss: 1.67982 Disc Loss: 0.417813\n",
      "Gen Loss: 1.79572 Disc Loss: 0.501947\n",
      "Gen Loss: 2.33794 Disc Loss: 0.486967\n",
      "Gen Loss: 2.07988 Disc Loss: 0.323634\n",
      "Gen Loss: 1.99961 Disc Loss: 0.358468\n",
      "Gen Loss: 1.28229 Disc Loss: 0.584663\n",
      "Gen Loss: 2.19787 Disc Loss: 0.363021\n",
      "Gen Loss: 2.79605 Disc Loss: 0.602894\n",
      "Gen Loss: 1.30867 Disc Loss: 0.468139\n",
      "Gen Loss: 1.7916 Disc Loss: 0.38528\n",
      "Gen Loss: 1.88197 Disc Loss: 0.57037\n",
      "Gen Loss: 2.09517 Disc Loss: 0.356229\n",
      "Gen Loss: 1.44014 Disc Loss: 0.366492\n",
      "Gen Loss: 1.99591 Disc Loss: 0.345923\n",
      "Gen Loss: 1.17165 Disc Loss: 0.649617\n",
      "Gen Loss: 3.90081 Disc Loss: 1.00675\n",
      "Gen Loss: 2.43506 Disc Loss: 0.447111\n",
      "Gen Loss: 1.29579 Disc Loss: 0.381811\n",
      "Gen Loss: 2.30513 Disc Loss: 0.265547\n",
      "Gen Loss: 1.11249 Disc Loss: 0.485335\n",
      "Gen Loss: 1.21674 Disc Loss: 0.688598\n",
      "Gen Loss: 0.830208 Disc Loss: 0.62546\n",
      "Gen Loss: 1.3046 Disc Loss: 0.446436\n",
      "Gen Loss: 2.27455 Disc Loss: 0.381147\n",
      "Gen Loss: 3.54699 Disc Loss: 0.995365\n",
      "Gen Loss: 0.476572 Disc Loss: 1.13066\n",
      "Gen Loss: 2.87405 Disc Loss: 0.535524\n",
      "Gen Loss: 2.05947 Disc Loss: 0.407781\n",
      "Gen Loss: 1.98621 Disc Loss: 0.46776\n",
      "Gen Loss: 2.31686 Disc Loss: 0.382168\n",
      "Gen Loss: 2.38023 Disc Loss: 0.244182\n",
      "Gen Loss: 1.34882 Disc Loss: 0.453381\n",
      "Gen Loss: 2.07991 Disc Loss: 0.543084\n",
      "Gen Loss: 2.11226 Disc Loss: 0.42596\n",
      "Gen Loss: 2.31559 Disc Loss: 0.463067\n",
      "Gen Loss: 1.95154 Disc Loss: 0.480885\n",
      "Gen Loss: 1.09034 Disc Loss: 0.386719\n",
      "Gen Loss: 2.12211 Disc Loss: 0.351959\n",
      "Gen Loss: 2.8768 Disc Loss: 0.216462\n",
      "Gen Loss: 1.96202 Disc Loss: 0.468511\n",
      "Gen Loss: 1.15662 Disc Loss: 0.654833\n",
      "Gen Loss: 1.61645 Disc Loss: 0.408549\n",
      "Gen Loss: 2.58724 Disc Loss: 0.707165\n",
      "Gen Loss: 1.99192 Disc Loss: 0.410412\n",
      "Gen Loss: 2.03729 Disc Loss: 0.469394\n",
      "Gen Loss: 1.70819 Disc Loss: 0.44771\n",
      "Gen Loss: 0.536163 Disc Loss: 1.13317\n",
      "Gen Loss: 2.37341 Disc Loss: 0.446968\n",
      "Gen Loss: 2.82126 Disc Loss: 0.536068\n",
      "Gen Loss: 2.20318 Disc Loss: 0.237695\n",
      "Gen Loss: 2.27083 Disc Loss: 0.257519\n",
      "Gen Loss: 2.83759 Disc Loss: 0.172812\n",
      "Gen Loss: 2.05742 Disc Loss: 0.397478\n",
      "Gen Loss: 2.32967 Disc Loss: 0.379571\n",
      "Gen Loss: 2.02575 Disc Loss: 0.374682\n",
      "Gen Loss: 2.05616 Disc Loss: 0.291903\n",
      "Gen Loss: 2.34434 Disc Loss: 0.297617\n",
      "Gen Loss: 1.32486 Disc Loss: 0.319207\n",
      "Gen Loss: 3.03628 Disc Loss: 0.485664\n",
      "Gen Loss: 1.91551 Disc Loss: 0.254068\n",
      "Gen Loss: 1.73435 Disc Loss: 0.449402\n",
      "Gen Loss: 1.82002 Disc Loss: 0.412506\n",
      "Gen Loss: 1.08025 Disc Loss: 0.352501\n",
      "Gen Loss: 1.67279 Disc Loss: 0.714083\n",
      "Gen Loss: 3.02556 Disc Loss: 0.538429\n",
      "Gen Loss: 1.59707 Disc Loss: 0.702637\n",
      "Gen Loss: 2.76178 Disc Loss: 0.877457\n",
      "Gen Loss: 2.38533 Disc Loss: 0.47086\n",
      "Gen Loss: 2.11595 Disc Loss: 0.28184\n",
      "Gen Loss: 2.83167 Disc Loss: 0.292598\n",
      "Gen Loss: 1.86615 Disc Loss: 0.305389\n",
      "Gen Loss: 1.94287 Disc Loss: 0.478063\n",
      "Gen Loss: 2.76517 Disc Loss: 0.53612\n",
      "Gen Loss: 2.17631 Disc Loss: 0.487761\n",
      "Saved Model\n",
      "Gen Loss: 1.79202 Disc Loss: 0.516066\n",
      "Gen Loss: 1.2977 Disc Loss: 0.760286\n",
      "Gen Loss: 1.98466 Disc Loss: 0.429565\n",
      "Gen Loss: 1.65044 Disc Loss: 0.62195\n",
      "Gen Loss: 1.20958 Disc Loss: 0.932389\n",
      "Gen Loss: 1.64193 Disc Loss: 0.511528\n",
      "Gen Loss: 2.69642 Disc Loss: 0.466331\n",
      "Gen Loss: 2.21718 Disc Loss: 0.390267\n",
      "Gen Loss: 2.17397 Disc Loss: 0.372206\n",
      "Gen Loss: 2.49876 Disc Loss: 0.489797\n",
      "Gen Loss: 1.33834 Disc Loss: 0.475745\n",
      "Gen Loss: 2.28373 Disc Loss: 0.473635\n",
      "Gen Loss: 2.27878 Disc Loss: 0.491379\n",
      "Gen Loss: 2.25212 Disc Loss: 0.309902\n",
      "Gen Loss: 1.87199 Disc Loss: 0.430273\n",
      "Gen Loss: 3.47204 Disc Loss: 0.942565\n",
      "Gen Loss: 2.30097 Disc Loss: 0.568335\n",
      "Gen Loss: 2.37167 Disc Loss: 0.654444\n",
      "Gen Loss: 2.33357 Disc Loss: 0.566766\n",
      "Gen Loss: 2.17132 Disc Loss: 0.366504\n",
      "Gen Loss: 2.77736 Disc Loss: 0.301038\n",
      "Gen Loss: 1.47177 Disc Loss: 0.501366\n",
      "Gen Loss: 2.14026 Disc Loss: 0.409543\n",
      "Gen Loss: 1.785 Disc Loss: 0.457246\n",
      "Gen Loss: 3.09703 Disc Loss: 0.743201\n",
      "Gen Loss: 1.7933 Disc Loss: 0.411563\n",
      "Gen Loss: 1.28234 Disc Loss: 0.723986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5c80d9fba777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;31m#Transform it to be between -1 and 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'constant'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Pad the images so the are 32x32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Update the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Update the generator, twice for good measure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\shuhua\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\shuhua\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\shuhua\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\shuhua\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\shuhua\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now that we have fully defined our network, it's time to train it\n",
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 500000 #Total number of iterations to use.\n",
    "sample_directory = 'figs' #Directory to save sample images from generator in.\n",
    "model_directory = 'models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0:\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using a trained network.\n",
    "Once we have a trained model saved, we may want to use it to generate new images. \n",
    "and explore the representation it has learned\n",
    "''' \n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 36\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/fig_generated.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
